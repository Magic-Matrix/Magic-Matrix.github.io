<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"magic-matrix.gitee.io","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":true},"copycode":{"enable":true,"show_result":true,"style":"default"},"back2top":{"enable":true,"sidebar":false,"scrollpercent":true},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="YOLO是“You Only Look Once”的缩写，YOLO将物体检测作为回归问题求解，是一个对象检测算法的名字，这是Redmon等人在2016年的一篇研究论文中命名的。  本篇文章会一步一步搭建起yolov3的神经网络，详细内容会具体分析。">
<meta property="og:type" content="article">
<meta property="og:title" content="用Pytorch搭建yolov3">
<meta property="og:url" content="https://magic-matrix.gitee.io/2021/02/06/%E7%94%A8Pytorch%E6%90%AD%E5%BB%BAyolov3/index.html">
<meta property="og:site_name" content="MatrixBlog">
<meta property="og:description" content="YOLO是“You Only Look Once”的缩写，YOLO将物体检测作为回归问题求解，是一个对象检测算法的名字，这是Redmon等人在2016年的一篇研究论文中命名的。  本篇文章会一步一步搭建起yolov3的神经网络，详细内容会具体分析。">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://magic-matrix.gitee.io/2021/02/06/%E7%94%A8Pytorch%E6%90%AD%E5%BB%BAyolov3/20210207195622.jpg">
<meta property="og:image" content="https://magic-matrix.gitee.io/2021/02/06/%E7%94%A8Pytorch%E6%90%AD%E5%BB%BAyolov3/20191020111518954.jpg">
<meta property="og:image" content="https://magic-matrix.gitee.io/2021/02/06/%E7%94%A8Pytorch%E6%90%AD%E5%BB%BAyolov3/2019.jpg">
<meta property="og:image" content="https://magic-matrix.gitee.io/2021/02/06/%E7%94%A8Pytorch%E6%90%AD%E5%BB%BAyolov3/1437686-20200102124439991-778619867.png">
<meta property="og:image" content="https://magic-matrix.gitee.io/2021/02/06/%E7%94%A8Pytorch%E6%90%AD%E5%BB%BAyolov3/1437686-20191230192708001-430020660.png">
<meta property="og:image" content="https://magic-matrix.gitee.io/2021/02/06/%E7%94%A8Pytorch%E6%90%AD%E5%BB%BAyolov3/10201.jpg">
<meta property="og:image" content="https://magic-matrix.gitee.io/2021/02/06/%E7%94%A8Pytorch%E6%90%AD%E5%BB%BAyolov3/0111.jpg">
<meta property="og:image" content="https://magic-matrix.gitee.io/2021/02/06/%E7%94%A8Pytorch%E6%90%AD%E5%BB%BAyolov3/1437686-20200102121153977-382469172.png">
<meta property="og:image" content="https://magic-matrix.gitee.io/2021/02/06/%E7%94%A8Pytorch%E6%90%AD%E5%BB%BAyolov3/20190509224534499.png">
<meta property="og:image" content="https://magic-matrix.gitee.io/2021/02/06/%E7%94%A8Pytorch%E6%90%AD%E5%BB%BAyolov3/8694a4c27d1ed21bc6ed15f6c38952c250da3fee.jpeg">
<meta property="article:published_time" content="2021-02-06T06:13:36.000Z">
<meta property="article:modified_time" content="2021-03-26T02:31:53.031Z">
<meta property="article:author" content="Matrix">
<meta property="article:tag" content="python">
<meta property="article:tag" content="深度学习">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://magic-matrix.gitee.io/2021/02/06/%E7%94%A8Pytorch%E6%90%AD%E5%BB%BAyolov3/20210207195622.jpg">

<link rel="canonical" href="https://magic-matrix.gitee.io/2021/02/06/%E7%94%A8Pytorch%E6%90%AD%E5%BB%BAyolov3/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>用Pytorch搭建yolov3 | MatrixBlog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>





    <!-- 页面点击小红心 -->
    <script type="text/javascript" src="/js/clicklove.js"></script>



<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">MatrixBlog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-fw fa-home"></i>首页</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-fw fa-tags"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-fw fa-th"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>归档</a>

  </li>
        <li class="menu-item menu-item-game">

    <a href="/game/" rel="section"><i class="fa fa-fw fa-legal"></i>游戏</a>

  </li>
        <li class="menu-item menu-item-recommend">

    <a href="/recommend/" rel="section"><i class="fa fa-fw fa-exchange"></i>推荐</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-fw fa-user"></i>关于</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://magic-matrix.gitee.io/2021/02/06/%E7%94%A8Pytorch%E6%90%AD%E5%BB%BAyolov3/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/header.jpg">
      <meta itemprop="name" content="Matrix">
      <meta itemprop="description" content="梦里不知身是客">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="MatrixBlog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          用Pytorch搭建yolov3
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2021-02-06 14:13:36" itemprop="dateCreated datePublished" datetime="2021-02-06T14:13:36+08:00">2021-02-06</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2021-03-26 10:31:53" itemprop="dateModified" datetime="2021-03-26T10:31:53+08:00">2021-03-26</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/" itemprop="url" rel="index"><span itemprop="name">人工智能</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>YOLO是“You Only Look Once”的缩写，YOLO将物体检测作为回归问题求解，是一个对象检测算法的名字，这是Redmon等人在2016年的一篇研究论文中命名的。</p>
<p><img src="/2021/02/06/%E7%94%A8Pytorch%E6%90%AD%E5%BB%BAyolov3/20210207195622.jpg" alt="20191020111518954" style="zoom:50%;"></p>
<p>本篇文章会一步一步搭建起yolov3的神经网络，详细内容会具体分析。</p>
<a id="more"></a>
<h2 id="YOLO介绍"><a href="#YOLO介绍" class="headerlink" title="YOLO介绍"></a>YOLO介绍</h2><p>目标检测（object detection）是一个因近年来深度学习的发展而受益颇多的领域，近年来，人们开发了多种目标检测算法，其中包括YOLO、SSD、Mask-RCNN和RetinaNet。此篇文章使用PyTorch并基于YOLO v3来实现一个目标检测器，这是一种速度更快的目标检测算法。</p>
<p>YOLO是“You Only Look Once”的缩写，YOLO将物体检测作为回归问题求解，是一个对象检测算法的名字，这是Redmon等人在2016年的一篇研究论文中命名的：</p>
<blockquote>
<p><strong><em>Redmon J , Divvala S , Girshick R , et al. You Only Look Once: Unified, Real-Time Object Detection[J]. 2015.</em></strong></p>
</blockquote>
<h2 id="YOLO结构"><a href="#YOLO结构" class="headerlink" title="YOLO结构"></a>YOLO结构</h2><p>以下为yolo的整体结构：</p>
<p><img src="/2021/02/06/%E7%94%A8Pytorch%E6%90%AD%E5%BB%BAyolov3/20191020111518954.jpg" alt="20191020111518954"></p>
<p>这张图分成两部分，左边虚线框起来的是<strong>主干特征提取</strong>部分。</p>
<h3 id="主干特征提取"><a href="#主干特征提取" class="headerlink" title="主干特征提取"></a>主干特征提取</h3><p>这部分称作Darknet-53，主干特征提取顾名思义，用来提取图片的特征。</p>
<p>输入是需要一张$416 <em> 416 </em> 3$大小的图片，然后是不断卷积的过程，如果仔细看会发现图片的高和宽不断被压缩，通道数却不断扩张，这是一个<strong>下采样</strong>过程。</p>
<p>经过下采样之后，会获得<strong>特征层</strong>（用来表述图片的特征），下图是我们需要的特征层。</p>
<p><img src="/2021/02/06/%E7%94%A8Pytorch%E6%90%AD%E5%BB%BAyolov3/2019.jpg" alt="2019"></p>
<p>由此可见保留下来的特征层为：$52<em>52</em>256$、$26<em>26</em>512$、$13<em>13</em>1024$三种尺寸，这三种特征层是即将传入右边的部分。</p>
<p><img src="/2021/02/06/%E7%94%A8Pytorch%E6%90%AD%E5%BB%BAyolov3/1437686-20200102124439991-778619867.png" alt="1437686-20200102124439991-778619867"></p>
<p>每个特征图可以看作一个“条目”，因为一个“条目”有一些信息，如下图所示：</p>
<p><img src="/2021/02/06/%E7%94%A8Pytorch%E6%90%AD%E5%BB%BAyolov3/1437686-20191230192708001-430020660.png" alt="1437686-20191230192708001-430020660"></p>
<h3 id="其他部分"><a href="#其他部分" class="headerlink" title="其他部分"></a>其他部分</h3><p>这里的其他部分泛指右边部分的网络。</p>
<p>经过主干特征提取后，我们获得了三种尺寸的特征层，分别对三种尺寸的特征层处理。</p>
<h4 id="处理-13131024-特征层"><a href="#处理-13131024-特征层" class="headerlink" title="处理$13131024$特征层"></a>处理$13<em>13</em>1024$特征层</h4><p>首先说一下$13<em>13</em>1024$特征层的处理：</p>
<p><img src="/2021/02/06/%E7%94%A8Pytorch%E6%90%AD%E5%BB%BAyolov3/10201.jpg" alt="10201"></p>
<p>可以看到$13<em>13</em>1024$特征层经过了5次卷积后传到了两个方向。</p>
<p>右边的粉色部分是<strong>分类预测</strong>和<strong>回归预测</strong>，其实就是两次卷积，最后会获得$13<em>13</em>75$大小的，再经过一个分解变成$13<em>13</em>3<em>25$，即$13</em>13<em>3</em>(20 + 1 + 4)$，这个过程其实就是化成$13<em>13$的网格，每个网格有<strong>3个</strong>预测出的<em>*先验框</em></em>，接下来会根据判断属于那种框的尺寸。</p>
<p>之所以会把25分成三部分，其实就gailv是20个物体分类，由于使用<strong>voc数据集</strong>，所以会有20个，<strong>coco训练集</strong>会出现80个，简单来讲20个<strong>置信度（属于哪个类的概率）</strong>会分类属于哪个类；1是指是否有物体；4是对框的调整。</p>
<p>注意$13<em>13</em>1024$特征层还有一个传递方向，这时需要一个<strong>上采样</strong>，也就是扩增长宽，减小通道数。经过上采样之后会和$26<em>26</em>512$特征层进行<strong>堆叠（增加通道数）</strong>。</p>
<h4 id="处理-2626512-特征层"><a href="#处理-2626512-特征层" class="headerlink" title="处理$2626512$特征层"></a>处理$26<em>26</em>512$特征层</h4><p>$26<em>26</em>512$特征层在和经过上采样的$13<em>13</em>1024$特征层进行堆叠后，会形成一个新的“特征金字塔”，当然这个特征金字塔还会继续堆叠。</p>
<p><img src="/2021/02/06/%E7%94%A8Pytorch%E6%90%AD%E5%BB%BAyolov3/0111.jpg" alt="0111"></p>
<p>后面的部分就一样了，再5次卷积，在最后进行分类预测和回归预测，会出现相同操作。</p>
<p>与此同时会把5次卷积后继续向上传递，并有个上采样。</p>
<p>$52<em>52</em>256$特征层同样如此就不再赘述。</p>
<p>输出结果：</p>
<p>$t_x$和$t_y$是被检测物体的中心位置，$t_w$和$t_h$是方框的尺寸，$p_0$是<strong>物体置信度</strong>，代表了这个区域内有物体的概率，$p_1-&gt;p_c$是分类置信度，哪类概率高，就属于什么物体。最后还有个B，这个是锚框个数，说明了这个区域可以最多检测出B个物体。</p>
<h3 id="Anchor-Boxes"><a href="#Anchor-Boxes" class="headerlink" title="Anchor Boxes"></a>Anchor Boxes</h3><p>有的翻译成锚框，有的翻译成先验框，这里我就叫它锚框了。</p>
<p>YOLO不能直接预测边界框的宽度和高度，这会导致训练过程中出现不稳定的变化。大多数现代目标检测器会预测对数空间转换，或者只是偏移到称为锚点的预定义默认锚框。YOLO-v3具有三个锚点，可在每个细胞单元格上预测三个边界框。</p>
<p>那么真正的方框是怎么预测出的？先看下面的公式：</p>
<script type="math/tex; mode=display">
b_x = \sigma(t_x) + c_x\\
b_y = \sigma(t_y) + c_y\\
b_w = p_w e^{t_w}\\
b_h = p_h e^{t_h}</script><p>前两个公式是预测中心点，经过$\sigma()$函数后就会稳定在0和1之间。$c_x$和$c_y$是第几个方框，例如上图中红色框的这两值都是6。</p>
<p>后面两个公式预测宽度，$p_w$和$p_h$是锚框的尺寸，直接乘就可以。</p>
<p>还没有完，这只完成了一半，接着看：</p>
<p><img src="/2021/02/06/%E7%94%A8Pytorch%E6%90%AD%E5%BB%BAyolov3/1437686-20200102121153977-382469172.png" alt="1437686-20200102121153977-382469172"></p>
<p>中心点需要还乘上对应的网格宽度，方框需要带入e指数中。</p>
<p>补一张更漂亮的图。</p>
<p><img src="/2021/02/06/%E7%94%A8Pytorch%E6%90%AD%E5%BB%BAyolov3/20190509224534499.png" alt="v2-fb8b964727ccfea93345ba1361c4c8a3_720w"></p>
<h2 id="YOLO预测原理"><a href="#YOLO预测原理" class="headerlink" title="YOLO预测原理"></a>YOLO预测原理</h2><p>YOLO将图片分成了3种检测，检测的区别是按照分割区域的大小。</p>
<p>首先输入图片的大小要定下为$416*416$大小，如果不合适就要补上缺失部分，目的就是防止失真。</p>
<p>将图片分别分成$52<em>52$、$26</em>26$、$13*13$、三种尺寸的网格，针对的识别三种尺寸，也就对应上了上部分的三种输出结果。</p>
<p>每个输出结果的维度为<code>(N, 75 * 3, x, x)</code>，x为尺寸分别对应52、26、13三种，N代表样本数，75 * 3这个数在上面提过了就不再多说。</p>
<p>最后的最终的预测结构后还要进行得分排序与非极大抑制筛选。</p>
<p>用$7<em>7$来举例（我只找到$7</em>7$的例子，我又懒得去做$13<em>13$的图），下面将一幅图分成$7</em>7$网格，共49部分。</p>
<p><img src="/2021/02/06/%E7%94%A8Pytorch%E6%90%AD%E5%BB%BAyolov3/8694a4c27d1ed21bc6ed15f6c38952c250da3fee.jpeg" alt="8694a4c27d1ed21bc6ed15f6c38952c250da3fee"></p>
<p>对于每个网格点，都会预测一个边界框和与每个类别（汽车，行人，交通信号灯等）相对应的概率，每个网络点负责一个区域的检测。</p>
<h2 id="程序"><a href="#程序" class="headerlink" title="程序"></a>程序</h2><p>首先搭建DarkNet大概框架：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DarkNet</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, layers</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(DarkNet, self).__init__()</span><br></pre></td></tr></table></figure>
<p>DarkNet继承了pytorch中的模型，目的使用一些相同的框架函数。</p>
<h3 id="初始的卷积层"><a href="#初始的卷积层" class="headerlink" title="初始的卷积层"></a>初始的卷积层</h3><p>继续根据整体结构来加入每一层，首先卷积层：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 设置卷积核个数，也是卷积后的通道数</span></span><br><span class="line">self.inplanes = <span class="number">32</span></span><br><span class="line"><span class="comment"># 大概配置参数是：</span></span><br><span class="line"><span class="comment"># 输入通道数、输出通道数（卷积核个数）、卷积核尺寸、步长填充、是否加入偏置</span></span><br><span class="line"><span class="comment"># 此时下面的配置是same卷积，没有破坏图像原本的大小</span></span><br><span class="line">self.conv1 = nn.Conv2d(<span class="number">3</span>, self.inplanes, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>, bias=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>
<p>标准化和激活函数：</p>
<p><strong>注意</strong>：这里使用的是LeakyReLU，这样好处是，负数区域还存在值，同时也有斜率。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># BatchNorm标准化，加速收敛速度及稳定性的算法</span></span><br><span class="line">self.bn1 = nn.BatchNorm2d(self.inplanes)</span><br><span class="line"><span class="comment"># 设置激活函数</span></span><br><span class="line">self.relu1 = nn.LeakyReLU(<span class="number">0.1</span>)</span><br></pre></td></tr></table></figure>
<p>以上操作建立了一个卷积层，最后生成的维度为：<code>(N, 416, 416, 32)</code></p>
<h3 id="残差块"><a href="#残差块" class="headerlink" title="残差块"></a>残差块</h3><p>先制做出残差块的函数，返回值是一个残差块：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_make_layer</span>(<span class="params">self, planes, blocks</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    制作一个结构块</span></span><br><span class="line"><span class="string">    :param planes: 是个列表，第一位置是输入通道数，第二位置是输出通道数</span></span><br><span class="line"><span class="string">    :param blocks: 残差块堆叠个数</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    layers = []</span><br><span class="line">    <span class="comment"># 下采样，步长为2，卷积核大小为3，填充为1</span></span><br><span class="line">    <span class="comment"># 这样长宽就可以压缩</span></span><br><span class="line">    layers.append((<span class="string">&quot;ds_conv&quot;</span>, nn.Conv2d(self.inplanes, planes[<span class="number">1</span>], kernel_size=<span class="number">3</span>,</span><br><span class="line">                            stride=<span class="number">2</span>, padding=<span class="number">1</span>, bias=<span class="literal">False</span>)))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># BatchNorm标准化</span></span><br><span class="line">    layers.append((<span class="string">&quot;ds_bn&quot;</span>, nn.BatchNorm2d(planes[<span class="number">1</span>])))</span><br><span class="line">    <span class="comment"># 设置激活函数</span></span><br><span class="line">    layers.append((<span class="string">&quot;ds_relu&quot;</span>, nn.LeakyReLU(<span class="number">0.1</span>)))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 加入darknet模块</span></span><br><span class="line">    self.inplanes = planes[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 堆叠残差块</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, blocks):</span><br><span class="line">        <span class="comment"># 添加一个网络</span></span><br><span class="line">        <span class="comment"># layers是个列表，里面元素是元组，元组元素是字符串和BasicBlock网络块</span></span><br><span class="line">        layers.append((<span class="string">&quot;residual_&#123;&#125;&quot;</span>.<span class="built_in">format</span>(i), BasicBlock(self.inplanes, planes)))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 打包网络</span></span><br><span class="line">    <span class="keyword">return</span> nn.Sequential(OrderedDict(layers))</span><br></pre></td></tr></table></figure>
<p>OrderedDict是有序字典，虽然平常不常用，但在这里使用是最合适的，记得导入包：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> OrderedDict</span><br></pre></td></tr></table></figure>
<p>最核心的部分是BasicBlock类，这里是残差网络相加的地方：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment"># 基本的darknet块（残差块的核心）</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BasicBlock</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, inplanes, planes</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        残差块</span></span><br><span class="line"><span class="string">        :param inplanes:</span></span><br><span class="line"><span class="string">        :param planes: 下采样部分</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="built_in">super</span>(BasicBlock, self).__init__()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 定义了两组卷积操作</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 最开始是卷积核形状为1的卷积</span></span><br><span class="line">        <span class="comment"># 目的是下降通道数</span></span><br><span class="line">        self.conv1 = nn.Conv2d(inplanes, planes[<span class="number">0</span>], kernel_size=<span class="number">1</span>,</span><br><span class="line">                               stride=<span class="number">1</span>, padding=<span class="number">0</span>, bias=<span class="literal">False</span>)</span><br><span class="line">        <span class="comment"># BatchNorm标准化</span></span><br><span class="line">        self.bn1 = nn.BatchNorm2d(planes[<span class="number">0</span>])</span><br><span class="line">        <span class="comment"># 设置激活函数</span></span><br><span class="line">        self.relu1 = nn.LeakyReLU(<span class="number">0.1</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 为了保证残差成功，必须使用same卷积</span></span><br><span class="line">        <span class="comment"># 这里还将通道数提升了</span></span><br><span class="line">        self.conv2 = nn.Conv2d(planes[<span class="number">0</span>], planes[<span class="number">1</span>], kernel_size=<span class="number">3</span>,</span><br><span class="line">                               stride=<span class="number">1</span>, padding=<span class="number">1</span>, bias=<span class="literal">False</span>)</span><br><span class="line">        <span class="comment"># BatchNorm标准化</span></span><br><span class="line">        self.bn2 = nn.BatchNorm2d(planes[<span class="number">1</span>])</span><br><span class="line">        <span class="comment"># 设置激活函数</span></span><br><span class="line">        self.relu2 = nn.LeakyReLU(<span class="number">0.1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        <span class="comment"># 保存残差”边“</span></span><br><span class="line">        residual = x</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 传统艺能</span></span><br><span class="line">        </span><br><span class="line">        out = self.conv1(x)</span><br><span class="line">        out = self.bn1(out)</span><br><span class="line">        out = self.relu1(out)</span><br><span class="line"></span><br><span class="line">        out = self.conv2(out)</span><br><span class="line">        out = self.bn2(out)</span><br><span class="line">        out = self.relu2(out)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 网络最后加上残差边</span></span><br><span class="line">        out += residual</span><br><span class="line">        <span class="keyword">return</span> out</span><br></pre></td></tr></table></figure>
<p>由于里面先进行1卷积，再3卷积，这样可以减少参数量，</p>
<p>初始化剩下的东西：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">   <span class="comment"># </span></span><br><span class="line">self.layers_out_filters = [<span class="number">64</span>, <span class="number">128</span>, <span class="number">256</span>, <span class="number">512</span>, <span class="number">1024</span>]</span><br><span class="line"></span><br><span class="line">   <span class="comment"># 进行权值初始化</span></span><br><span class="line">   <span class="keyword">for</span> m <span class="keyword">in</span> self.modules():</span><br><span class="line">       <span class="keyword">if</span> <span class="built_in">isinstance</span>(m, nn.Conv2d):</span><br><span class="line">           n = m.kernel_size[<span class="number">0</span>] * m.kernel_size[<span class="number">1</span>] * m.out_channels</span><br><span class="line">           m.weight.data.normal_(<span class="number">0</span>, math.sqrt(<span class="number">2.</span> / n))</span><br><span class="line">       <span class="keyword">elif</span> <span class="built_in">isinstance</span>(m, nn.BatchNorm2d):</span><br><span class="line">           m.weight.data.fill_(<span class="number">1</span>)</span><br><span class="line">           m.bias.data.zero_()</span><br></pre></td></tr></table></figure>
<h3 id="前向传播"><a href="#前向传播" class="headerlink" title="前向传播"></a>前向传播</h3><p>前向传播的时候我们需要返回三个尺寸的特征图，所以需要这样写：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    前向传播</span></span><br><span class="line"><span class="string">    :param x: 输入图</span></span><br><span class="line"><span class="string">    :return: 三个特征图</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 传统艺能</span></span><br><span class="line">    x = self.conv1(x)</span><br><span class="line">    x = self.bn1(x)</span><br><span class="line">    x = self.relu1(x)</span><br><span class="line"></span><br><span class="line">    x = self.layer1(x)</span><br><span class="line">    x = self.layer2(x)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 第一特征图</span></span><br><span class="line">    out3 = self.layer3(x)</span><br><span class="line">    <span class="comment"># 第二特征图</span></span><br><span class="line">    out4 = self.layer4(out3)</span><br><span class="line">    <span class="comment"># 第三特征图</span></span><br><span class="line">    out5 = self.layer5(out4)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> out3, out4, out5</span><br></pre></td></tr></table></figure>
<h3 id="从特征获取预测结果"><a href="#从特征获取预测结果" class="headerlink" title="从特征获取预测结果"></a>从特征获取预测结果</h3><p>特征图出来了，但我们需要将特征图转换成最终结果。</p>
<p>在放代码之前，自定义一套卷积，这是方便后面快速使用：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">conv2d</span>(<span class="params">filter_in, filter_out, kernel_size</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    私自定义的卷积</span></span><br><span class="line"><span class="string">    :param filter_in: 输入通道数</span></span><br><span class="line"><span class="string">    :param filter_out: 输出通道数</span></span><br><span class="line"><span class="string">    :param kernel_size: 卷积核尺寸</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    pad = (kernel_size - <span class="number">1</span>) // <span class="number">2</span> <span class="keyword">if</span> kernel_size <span class="keyword">else</span> <span class="number">0</span></span><br><span class="line">    <span class="keyword">return</span> nn.Sequential(OrderedDict([</span><br><span class="line">        (<span class="string">&quot;conv&quot;</span>, nn.Conv2d(filter_in, filter_out, kernel_size=kernel_size, stride=<span class="number">1</span>, padding=pad, bias=<span class="literal">False</span>)),</span><br><span class="line">        (<span class="string">&quot;bn&quot;</span>, nn.BatchNorm2d(filter_out)),</span><br><span class="line">        (<span class="string">&quot;relu&quot;</span>, nn.LeakyReLU(<span class="number">0.1</span>)),</span><br><span class="line">    ]))</span><br></pre></td></tr></table></figure>
<p>这个卷积相当于一套完整的卷积层，但官方这里没有使用残差网络，这有点让我感到疑惑，因为我担心这样会不会影响整个网络前部分的梯度计算。</p>
<h4 id="主体部分的初始化"><a href="#主体部分的初始化" class="headerlink" title="主体部分的初始化"></a>主体部分的初始化</h4><p>因为是主体部分，所以首先创建yolo主体：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">YoloBody</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, config</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        yolo主体</span></span><br><span class="line"><span class="string">        :param config:</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="built_in">super</span>(YoloBody, self).__init__()</span><br><span class="line">        self.config = config</span><br></pre></td></tr></table></figure>
<p>然后获取已经创建好的网络（也就是上面的程序）：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 获取darknet53堆叠网络</span></span><br><span class="line">self.backbone = darknet53(<span class="literal">None</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取输出通道数</span></span><br><span class="line">out_filters = self.backbone.layers_out_filters</span><br></pre></td></tr></table></figure>
<p>接下来就是$13*13$特征层的提取：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># last_layer0</span></span><br><span class="line"><span class="comment"># 此值为75</span></span><br><span class="line">final_out_filter0 = <span class="built_in">len</span>(config[<span class="string">&quot;yolo&quot;</span>][<span class="string">&quot;anchors&quot;</span>][<span class="number">0</span>]) * (<span class="number">5</span> + config[<span class="string">&quot;yolo&quot;</span>][<span class="string">&quot;classes&quot;</span>])</span><br><span class="line"><span class="comment"># 5次卷积+2次卷积</span></span><br><span class="line">self.last_layer0 = make_last_layers([<span class="number">512</span>, <span class="number">1024</span>], out_filters[-<span class="number">1</span>], final_out_filter0)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>现在详细说一下<code>final_out_filter0</code>这个变量特点，此变量分两部分，两部分相乘才出结果。<code>len(config[&quot;yolo&quot;][&quot;anchors&quot;][0])</code>是<strong>先验框</strong>个数，我们使用了3个，所以此结果是3；<code>(5 + config[&quot;yolo&quot;][&quot;classes&quot;])</code>这个是$20 + 1 + 4$，也就是25。最终结果就是$(20+1+4)*3=75$。</p>
<p>这个时候有个<code>make_last_layers</code>函数，这就是5次卷积和最后结果卷积。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">make_last_layers</span>(<span class="params">filters_list, in_filters, out_filter</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    5次卷积+2次卷积</span></span><br><span class="line"><span class="string">    :param filters_list:中间过渡通道数</span></span><br><span class="line"><span class="string">    :param in_filters: 输入通道数</span></span><br><span class="line"><span class="string">    :param out_filter: 输出通道数</span></span><br><span class="line"><span class="string">    :return: 5次卷积+2次卷积的模型</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    m = nn.ModuleList([</span><br><span class="line">        <span class="comment"># 1*1卷积调整通道（降低通道数）</span></span><br><span class="line">        conv2d(in_filters, filters_list[<span class="number">0</span>], <span class="number">1</span>),</span><br><span class="line">        <span class="comment"># 3*3卷积提取特征</span></span><br><span class="line">        conv2d(filters_list[<span class="number">0</span>], filters_list[<span class="number">1</span>], <span class="number">3</span>),</span><br><span class="line">        <span class="comment"># 1*1卷积调整通道（降低通道数）</span></span><br><span class="line">        conv2d(filters_list[<span class="number">1</span>], filters_list[<span class="number">0</span>], <span class="number">1</span>),</span><br><span class="line">        <span class="comment"># 3*3卷积提取特征</span></span><br><span class="line">        conv2d(filters_list[<span class="number">0</span>], filters_list[<span class="number">1</span>], <span class="number">3</span>),</span><br><span class="line">        <span class="comment"># 1*1卷积调整通道</span></span><br><span class="line">        conv2d(filters_list[<span class="number">1</span>], filters_list[<span class="number">0</span>], <span class="number">1</span>),</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 接下来两此卷积用来分类预测和回归预测</span></span><br><span class="line">        conv2d(filters_list[<span class="number">0</span>], filters_list[<span class="number">1</span>], <span class="number">3</span>),</span><br><span class="line">        nn.Conv2d(filters_list[<span class="number">1</span>], out_filter, kernel_size=<span class="number">1</span>,</span><br><span class="line">                                        stride=<span class="number">1</span>, padding=<span class="number">0</span>, bias=<span class="literal">True</span>)</span><br><span class="line">    ])</span><br><span class="line">    <span class="keyword">return</span> m</span><br></pre></td></tr></table></figure>
<p>$1*1$卷积是很有效减少通道数，从而减少参数，这对电脑减轻了不小负担。</p>
<p>接下来是$13*13$特征层的提取：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># embedding1</span></span><br><span class="line"><span class="comment"># 此值为75</span></span><br><span class="line">final_out_filter1 = <span class="built_in">len</span>(config[<span class="string">&quot;yolo&quot;</span>][<span class="string">&quot;anchors&quot;</span>][<span class="number">1</span>]) * (<span class="number">5</span> + config[<span class="string">&quot;yolo&quot;</span>][<span class="string">&quot;classes&quot;</span>])</span><br><span class="line"><span class="comment"># 1*1卷积调整通道数（从512调节成256通道）</span></span><br><span class="line">self.last_layer1_conv = conv2d(<span class="number">512</span>, <span class="number">256</span>, <span class="number">1</span>)</span><br><span class="line"><span class="comment"># 上采样</span></span><br><span class="line">self.last_layer1_upsample = nn.Upsample(scale_factor=<span class="number">2</span>, mode=<span class="string">&#x27;nearest&#x27;</span>)</span><br><span class="line"><span class="comment"># 5次卷积+2次卷积</span></span><br><span class="line">self.last_layer1 = make_last_layers([<span class="number">256</span>, <span class="number">512</span>], out_filters[-<span class="number">2</span>] + <span class="number">256</span>, final_out_filter1)</span><br></pre></td></tr></table></figure>
<p>在使用上采样之前需要用$1*1$卷积核来调整通道，这样可以保证接下来上采样时通道一致。</p>
<p>由于使用上采样，所以我们需要上采样函数，幸运的是pytorch提供了上采样，所以我们免去了这个过程。</p>
<p>最后就是剩下特征层，也是同样道理：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># embedding2</span></span><br><span class="line"><span class="comment"># 此值为75</span></span><br><span class="line">final_out_filter2 = <span class="built_in">len</span>(config[<span class="string">&quot;yolo&quot;</span>][<span class="string">&quot;anchors&quot;</span>][<span class="number">2</span>]) * (<span class="number">5</span> + config[<span class="string">&quot;yolo&quot;</span>][<span class="string">&quot;classes&quot;</span>])</span><br><span class="line"><span class="comment"># 1*1卷积调整通道数（从256调节成128通道）</span></span><br><span class="line">self.last_layer2_conv = conv2d(<span class="number">256</span>, <span class="number">128</span>, <span class="number">1</span>)</span><br><span class="line"><span class="comment"># 上采样</span></span><br><span class="line">self.last_layer2_upsample = nn.Upsample(scale_factor=<span class="number">2</span>, mode=<span class="string">&#x27;nearest&#x27;</span>)</span><br><span class="line"><span class="comment"># 5次卷积+2次卷积</span></span><br><span class="line">self.last_layer2 = make_last_layers([<span class="number">128</span>, <span class="number">256</span>], out_filters[-<span class="number">3</span>] + <span class="number">128</span>, final_out_filter2)</span><br></pre></td></tr></table></figure>
<h4 id="前向传播-1"><a href="#前向传播-1" class="headerlink" title="前向传播"></a>前向传播</h4><p>看到这里，所有初始化都完成了，也就是网络结构部分完成了，剩下就是其他的代码，就比如前向传播的整体还没写。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    前向传播</span></span><br><span class="line"><span class="string">    :param x: 输入</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 函数中定义函数</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_branch</span>(<span class="params">last_layer, layer_in</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        分开卷积过程</span></span><br><span class="line"><span class="string">        :param last_layer: 上一层结果</span></span><br><span class="line"><span class="string">        :param layer_in:会存入卷积过程的结果并会返回出去</span></span><br><span class="line"><span class="string">        :return: out_branch是最后结果</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">for</span> i, e <span class="keyword">in</span> <span class="built_in">enumerate</span>(last_layer):</span><br><span class="line">            layer_in = e(layer_in)</span><br><span class="line">            <span class="keyword">if</span> i == <span class="number">4</span>:</span><br><span class="line">                <span class="comment"># 第五次卷积的时候取出来</span></span><br><span class="line">                out_branch = layer_in</span><br><span class="line">        <span class="keyword">return</span> layer_in, out_branch</span><br></pre></td></tr></table></figure>
<p>没错，直接套娃就可以，我挺喜欢这样写，因为这样写可以让整个函数在另一个函数周期里，外函数“死亡”时，里面的函数也会“死亡”。之所以定义这个函数是把第五次卷积结果拿出来，因为需要上采样传递给其他层。</p>
<p>继续写，在DarkNet中我们可以获得到三种特征图。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 获取三种特征图</span></span><br><span class="line">x2, x1, x0 = self.backbone(x)</span><br></pre></td></tr></table></figure>
<p>现在可以正常正向传播了，首先第一套特征图：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 5次卷积+2次卷积</span></span><br><span class="line"><span class="comment"># 直接可以得到卷积神经网络结果</span></span><br><span class="line">out0, out0_branch = _branch(self.last_layer0, x0)</span><br></pre></td></tr></table></figure>
<p>第二套特征图使用计算之前，需要调整通道数、上采样、堆叠：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1*1卷积调整通道数（从512调节成256通道）</span></span><br><span class="line">x1_in = self.last_layer1_conv(out0_branch)</span><br><span class="line"><span class="comment"># 上采样</span></span><br><span class="line">x1_in = self.last_layer1_upsample(x1_in)</span><br><span class="line"><span class="comment"># 堆叠</span></span><br><span class="line">x1_in = torch.cat([x1_in, x1], <span class="number">1</span>)</span><br><span class="line"><span class="comment"># 5次卷积+2次卷积</span></span><br><span class="line"><span class="comment"># 直接可以得到卷积神经网络结果</span></span><br><span class="line">out1, out1_branch = _branch(self.last_layer1, x1_in)</span><br></pre></td></tr></table></figure>
<p>第三套同样如此：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1*1卷积调整通道数（从256调节成128通道）</span></span><br><span class="line">x2_in = self.last_layer2_conv(out1_branch)</span><br><span class="line"><span class="comment"># 上采样</span></span><br><span class="line">x2_in = self.last_layer2_upsample(x2_in)</span><br><span class="line"><span class="comment"># 堆叠</span></span><br><span class="line">x2_in = torch.cat([x2_in, x2], <span class="number">1</span>)</span><br><span class="line"><span class="comment"># 5次卷积+2次卷积</span></span><br><span class="line"><span class="comment"># 直接可以得到卷积神经网络结果</span></span><br><span class="line">out2, _ = _branch(self.last_layer2, x2_in)        </span><br></pre></td></tr></table></figure>
<p>最后直接输出：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">return</span> out0, out1, out2</span><br></pre></td></tr></table></figure>
<p>以上我们的网络就搭建完毕了，但还缺少解码部分以及损失函数，所以继续写吧。</p>
<h3 id="解码"><a href="#解码" class="headerlink" title="解码"></a>解码</h3><p>将解码也定义一个层，这样就方便后续的处理：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DecodeBox</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, anchors, num_classes, img_size</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(DecodeBox, self).__init__()</span><br><span class="line">        self.anchors = anchors</span><br><span class="line">        self.num_anchors = <span class="built_in">len</span>(anchors)</span><br><span class="line">        self.num_classes = num_classes</span><br><span class="line">        self.bbox_attrs = <span class="number">5</span> + num_classes</span><br><span class="line">        <span class="comment"># 保存图像大小</span></span><br><span class="line">        self.img_size = img_size</span><br></pre></td></tr></table></figure>
<p>前向传播部分：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, <span class="built_in">input</span></span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 此时的形状是（N, 3*(20+5), size, size）</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 获取图片数量（N,）</span></span><br><span class="line">    batch_size = <span class="built_in">input</span>.size(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 获取图片几行网格</span></span><br><span class="line">    input_height = <span class="built_in">input</span>.size(<span class="number">2</span>)</span><br><span class="line">    <span class="comment"># 获取图片几列网格</span></span><br><span class="line">    input_width = <span class="built_in">input</span>.size(<span class="number">3</span>)</span><br></pre></td></tr></table></figure>
<p>当然还没完，我们还要获得图片步长，也就是图片每个网格内有多少像素点（感受野）：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 计算步长</span></span><br><span class="line"><span class="comment"># 每个网格内有多少像素点</span></span><br><span class="line"><span class="comment"># 图片尺寸 / 每个网格大小</span></span><br><span class="line">stride_h = self.img_size[<span class="number">1</span>] / input_height</span><br><span class="line">stride_w = self.img_size[<span class="number">0</span>] / input_width</span><br></pre></td></tr></table></figure>
<p>我们之前定义了三种先验框，所以注意最开始的先验框的单位是像素，我们需要将这个改变成根据步长的百分比，也就是归一到特征层上：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 归一到特征层上</span></span><br><span class="line">scaled_anchors = [(anchor_width / stride_w, anchor_height / stride_h) <span class="keyword">for</span> anchor_width, anchor_height <span class="keyword">in</span> self.anchors]</span><br></pre></td></tr></table></figure>
<p>因为有三套先验框，所以这个循环会执行三次。</p>
<p>还需要通道转换，<code>.permute(0, 1, 3, 4, 2).contiguous()</code>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># （N, 3*(20+5), size, size）-&gt;（N, 3 , size, size, 20+5）</span></span><br><span class="line">prediction = <span class="built_in">input</span>.view(batch_size, self.num_anchors,</span><br><span class="line">                        self.bbox_attrs, input_height, input_width).permute(<span class="number">0</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">2</span>).contiguous()</span><br></pre></td></tr></table></figure>
<p>处理一下先验框的中心参数，怕是遇到负数，所以使用<code>sigmoid()</code>函数过滤一下，这样百分百是正数且不会大于1：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 先验框的中心位置的调整参数</span></span><br><span class="line">x = torch.sigmoid(prediction[..., <span class="number">0</span>])</span><br><span class="line">y = torch.sigmoid(prediction[..., <span class="number">1</span>])</span><br></pre></td></tr></table></figure>
<p>还有宽高参数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 先验框的宽高调整参数</span></span><br><span class="line">w = prediction[..., <span class="number">2</span>]  <span class="comment"># Width</span></span><br><span class="line">h = prediction[..., <span class="number">3</span>]  <span class="comment"># Height</span></span><br></pre></td></tr></table></figure>
<p>以及物体置信度和种类置信度，听说在之前yolo使用的是<code>softmax()</code>函数，这样就会产生独立事件，有时会出bug，<code>sigmoid()</code>可以解决这个问题：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 获得置信度，是否有物体</span></span><br><span class="line">conf = torch.sigmoid(prediction[..., <span class="number">4</span>])</span><br><span class="line"><span class="comment"># 获得种类置信度</span></span><br><span class="line">pred_cls = torch.sigmoid(prediction[..., <span class="number">5</span>:])</span><br></pre></td></tr></table></figure>
<p>先验框问题解决了，接下来就生成出来，根据网格左上角生成中心：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 生成网格，先验框中心，网格左上角</span></span><br><span class="line">grid_x = torch.linspace(<span class="number">0</span>, input_width - <span class="number">1</span>, input_width).repeat(input_width, <span class="number">1</span>).repeat(batch_size * self.num_anchors, <span class="number">1</span>, <span class="number">1</span>).view(x.shape).<span class="built_in">type</span>(FloatTensor)</span><br><span class="line">grid_y = torch.linspace(<span class="number">0</span>, input_height - <span class="number">1</span>, input_height).repeat(input_height, <span class="number">1</span>).t().repeat(batch_size * self.num_anchors, <span class="number">1</span>, <span class="number">1</span>).view(y.shape).<span class="built_in">type</span>(FloatTensor)</span><br></pre></td></tr></table></figure>
<p>也就是从网格的左上角，每隔size单位生成一个，形状是<code>（N, 3, size, size）</code></p>
<p>再生成先验框的宽和高，根据自己设定的原始先验框：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 生成先验框的宽高</span></span><br><span class="line">anchor_w = FloatTensor(scaled_anchors).index_select(<span class="number">1</span>, LongTensor([<span class="number">0</span>]))</span><br><span class="line">anchor_h = FloatTensor(scaled_anchors).index_select(<span class="number">1</span>, LongTensor([<span class="number">1</span>]))</span><br><span class="line">anchor_w = anchor_w.repeat(batch_size, <span class="number">1</span>).repeat(<span class="number">1</span>, <span class="number">1</span>, input_height * input_width).view(w.shape)</span><br><span class="line">anchor_h = anchor_h.repeat(batch_size, <span class="number">1</span>).repeat(<span class="number">1</span>, <span class="number">1</span>, input_height * input_width).view(h.shape)</span><br></pre></td></tr></table></figure>
<p>这是最后的调整了：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 计算调整后的先验框中心与宽高</span></span><br><span class="line">pred_boxes = FloatTensor(prediction[..., :<span class="number">4</span>].shape)</span><br><span class="line">pred_boxes[..., <span class="number">0</span>] = x.data + grid_x</span><br><span class="line">pred_boxes[..., <span class="number">1</span>] = y.data + grid_y</span><br><span class="line">pred_boxes[..., <span class="number">2</span>] = torch.exp(w.data) * anchor_w</span><br><span class="line">pred_boxes[..., <span class="number">3</span>] = torch.exp(h.data) * anchor_h</span><br></pre></td></tr></table></figure>
<p>之前我们获得的先验框中心是一个0到1之间的数字，加上网格的次序就可以直接检查出是哪个网格中的中心。</p>
<p>结尾：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 用于将输出调整为相对于416x416的大小</span></span><br><span class="line">_scale = torch.Tensor([stride_w, stride_h] * <span class="number">2</span>).<span class="built_in">type</span>(FloatTensor)</span><br><span class="line">output = torch.cat((pred_boxes.view(batch_size, -<span class="number">1</span>, <span class="number">4</span>) * _scale,conf.view(batch_size, -<span class="number">1</span>, <span class="number">1</span>), pred_cls.view(batch_size, -<span class="number">1</span>, self.num_classes)), -<span class="number">1</span>)</span><br><span class="line"><span class="keyword">return</span> output.data</span><br></pre></td></tr></table></figure>
<blockquote>
<p>参考：</p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_44791964/article/details/105310627">https://blog.csdn.net/weixin_44791964/article/details/105310627</a></p>
<p><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1Hp4y1y788">https://www.bilibili.com/video/BV1Hp4y1y788</a></p>
</blockquote>

    </div>

    
    
    
        <div class="reward-container">
  <div></div>
  <button onclick="var qr = document.getElementById('qr'); qr.style.display = (qr.style.display === 'none') ? 'block' : 'none';">
    打赏
  </button>
  <div id="qr" style="display: none;">
      
      <div style="display: inline-block;">
        <img src="/images/wechatpay.png" alt="Matrix 微信支付">
        <p>微信支付</p>
      </div>
      
      <div style="display: inline-block;">
        <img src="/images/alipay.jpg" alt="Matrix 支付宝">
        <p>支付宝</p>
      </div>

  </div>
</div>


      <footer class="post-footer">
          
          <div class="post-tags">
              <a href="/tags/python/" rel="tag"><i class="fa fa-tag"></i> python</a>
              <a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" rel="tag"><i class="fa fa-tag"></i> 深度学习</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2021/02/02/%E5%B0%8F%E7%B1%B3mini%E8%B7%AF%E7%94%B1%E5%99%A8%E5%88%B7openwrt/" rel="prev" title="小米mini路由器刷openwrt">
      <i class="fa fa-chevron-left"></i> 小米mini路由器刷openwrt
    </a></div>
      <div class="post-nav-item">
    <a href="/2021/02/10/%E5%AF%B9%E8%AF%9D%E6%B8%B8%E6%88%8F/" rel="next" title="对话游戏">
      对话游戏 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#YOLO%E4%BB%8B%E7%BB%8D"><span class="nav-number">1.</span> <span class="nav-text">YOLO介绍</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#YOLO%E7%BB%93%E6%9E%84"><span class="nav-number">2.</span> <span class="nav-text">YOLO结构</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%BB%E5%B9%B2%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96"><span class="nav-number">2.1.</span> <span class="nav-text">主干特征提取</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%85%B6%E4%BB%96%E9%83%A8%E5%88%86"><span class="nav-number">2.2.</span> <span class="nav-text">其他部分</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%A4%84%E7%90%86-13131024-%E7%89%B9%E5%BE%81%E5%B1%82"><span class="nav-number">2.2.1.</span> <span class="nav-text">处理$13131024$特征层</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%A4%84%E7%90%86-2626512-%E7%89%B9%E5%BE%81%E5%B1%82"><span class="nav-number">2.2.2.</span> <span class="nav-text">处理$2626512$特征层</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Anchor-Boxes"><span class="nav-number">2.3.</span> <span class="nav-text">Anchor Boxes</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#YOLO%E9%A2%84%E6%B5%8B%E5%8E%9F%E7%90%86"><span class="nav-number">3.</span> <span class="nav-text">YOLO预测原理</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%A8%8B%E5%BA%8F"><span class="nav-number">4.</span> <span class="nav-text">程序</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%88%9D%E5%A7%8B%E7%9A%84%E5%8D%B7%E7%A7%AF%E5%B1%82"><span class="nav-number">4.1.</span> <span class="nav-text">初始的卷积层</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%AE%8B%E5%B7%AE%E5%9D%97"><span class="nav-number">4.2.</span> <span class="nav-text">残差块</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%89%8D%E5%90%91%E4%BC%A0%E6%92%AD"><span class="nav-number">4.3.</span> <span class="nav-text">前向传播</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BB%8E%E7%89%B9%E5%BE%81%E8%8E%B7%E5%8F%96%E9%A2%84%E6%B5%8B%E7%BB%93%E6%9E%9C"><span class="nav-number">4.4.</span> <span class="nav-text">从特征获取预测结果</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%B8%BB%E4%BD%93%E9%83%A8%E5%88%86%E7%9A%84%E5%88%9D%E5%A7%8B%E5%8C%96"><span class="nav-number">4.4.1.</span> <span class="nav-text">主体部分的初始化</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%89%8D%E5%90%91%E4%BC%A0%E6%92%AD-1"><span class="nav-number">4.4.2.</span> <span class="nav-text">前向传播</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%A7%A3%E7%A0%81"><span class="nav-number">4.5.</span> <span class="nav-text">解码</span></a></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Matrix"
      src="/images/header.jpg">
  <p class="site-author-name" itemprop="name">Matrix</p>
  <div class="site-description" itemprop="description">梦里不知身是客</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">19</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">8</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">16</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/Magic-Matrix" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;Magic-Matrix" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://space.bilibili.com/97371740" title="Bilibili → https:&#x2F;&#x2F;space.bilibili.com&#x2F;97371740" rel="noopener" target="_blank"><i class="fa fa-fw fa-laptop"></i>Bilibili</a>
      </span>
  </div>



      </div>
<iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width=330 height=86 src="//music.163.com/outchain/player?type=2&id=1391259110&auto=0&height=66"></iframe>
    </div>

  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Matrix</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>


  <script defer src="/lib/three/three.min.js"></script>


  















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
          load: ['[tex]/mhchem'],
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
          packages: {'[+]': ['mhchem']},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  


</body>
</html>
