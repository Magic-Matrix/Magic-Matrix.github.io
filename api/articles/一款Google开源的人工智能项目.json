{"title":"一款Google开源的人工智能项目","slug":"一款Google开源的人工智能项目","date":"2021-10-16T09:08:00.000Z","updated":"2021-10-16T10:24:42.510Z","comments":true,"path":"api/articles/一款Google开源的人工智能项目.json","excerpt":null,"covers":["https://google.github.io/mediapipe/images/mediapipe_small.png","https://google.github.io/mediapipe/images/mobile/face_detection_android_gpu_small.gif","https://google.github.io/mediapipe/images/mobile/face_mesh_android_gpu_small.gif","https://google.github.io/mediapipe/images/mobile/iris_tracking_android_gpu_small.gif","https://google.github.io/mediapipe/images/mobile/hand_tracking_android_gpu_small.gif","https://google.github.io/mediapipe/images/mobile/pose_tracking_android_gpu_small.gif","https://google.github.io/mediapipe/images/mobile/holistic_tracking_android_gpu_small.gif","https://google.github.io/mediapipe/images/mobile/hair_segmentation_android_gpu_small.gif","https://google.github.io/mediapipe/images/mobile/object_detection_android_gpu_small.gif","https://google.github.io/mediapipe/images/mobile/object_tracking_android_gpu_small.gif","https://google.github.io/mediapipe/images/mobile/instant_motion_tracking_android_small.gif","https://google.github.io/mediapipe/images/mobile/objectron_chair_android_gpu_small.gif","https://google.github.io/mediapipe/images/mobile/template_matching_android_cpu_small.gif","https://google.github.io/mediapipe/images/mobile/hand_landmarks.png","https://google.github.io/mediapipe/images/mobile/hand_crops.png"],"content":"<p>今天介绍一款Google开源的人工智能项目——Mediapipe。</p>\n<p><img src=\"https://google.github.io/mediapipe/images/mediapipe_small.png\" alt=\"MediaPipe\"></p>\n<p>在这里放出此项目的官网链接：<a href=\"https://google.github.io/mediapipe/\">【点击此处进入官网】</a></p>\n<p>Mediapipe是一个基于图的数据处理管线，用于构建使用了多种形式的数据源，如视频、音频、传感器数据以及任何时间序列数据。</p>\n<p>下面是提供的该项目名单：</p>\n<div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th style=\"text-align:center\"><a href=\"https://google.github.io/mediapipe/solutions/face_detection\"><strong>Face Detection</strong>)</a></th>\n<th style=\"text-align:center\"><a href=\"https://google.github.io/mediapipe/solutions/face_mesh\"><strong>Face Mesh</strong></a></th>\n<th style=\"text-align:center\"><a href=\"https://google.github.io/mediapipe/solutions/iris\"><strong>Iris</strong></a></th>\n<th style=\"text-align:center\"><a href=\"https://google.github.io/mediapipe/solutions/hands\"><strong>Hands</strong></a></th>\n<th style=\"text-align:center\"><a href=\"https://google.github.io/mediapipe/solutions/pose\"><strong>Pose</strong></a></th>\n<th style=\"text-align:center\"><a href=\"https://google.github.io/mediapipe/solutions/holistic\"><strong>Holistic</strong></a></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\"><img src=\"https://google.github.io/mediapipe/images/mobile/face_detection_android_gpu_small.gif\" alt=\"face_detection\"></td>\n<td style=\"text-align:center\"><img src=\"https://google.github.io/mediapipe/images/mobile/face_mesh_android_gpu_small.gif\" alt=\"face_mesh\"></td>\n<td style=\"text-align:center\"><img src=\"https://google.github.io/mediapipe/images/mobile/iris_tracking_android_gpu_small.gif\" alt=\"iris\"></td>\n<td style=\"text-align:center\"><img src=\"https://google.github.io/mediapipe/images/mobile/hand_tracking_android_gpu_small.gif\" alt=\"hand\"></td>\n<td style=\"text-align:center\"><img src=\"https://google.github.io/mediapipe/images/mobile/pose_tracking_android_gpu_small.gif\" alt=\"pose\"></td>\n<td style=\"text-align:center\"><img src=\"https://google.github.io/mediapipe/images/mobile/holistic_tracking_android_gpu_small.gif\" alt=\"hair_segmentation\"></td>\n</tr>\n</tbody>\n</table>\n</div>\n<div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th style=\"text-align:center\"><a href=\"https://google.github.io/mediapipe/solutions/hair_segmentation\"><strong>Hair Segmentation</strong></a></th>\n<th style=\"text-align:center\"><a href=\"https://google.github.io/mediapipe/solutions/object_detection\"><strong>Object Detection</strong></a></th>\n<th style=\"text-align:center\"><a href=\"https://google.github.io/mediapipe/solutions/box_tracking\"><strong>Box Tracking</strong></a></th>\n<th style=\"text-align:center\"><a href=\"https://google.github.io/mediapipe/solutions/instant_motion_tracking\"><strong>Instant Motion Tracking</strong></a></th>\n<th style=\"text-align:center\"><a href=\"https://google.github.io/mediapipe/solutions/objectron\"><strong>Objectron</strong></a></th>\n<th style=\"text-align:center\"><a href=\"https://google.github.io/mediapipe/solutions/knift\"><strong>KNIFT</strong></a></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\"><img src=\"https://google.github.io/mediapipe/images/mobile/hair_segmentation_android_gpu_small.gif\" alt=\"hair_segmentation\"></td>\n<td style=\"text-align:center\"><img src=\"https://google.github.io/mediapipe/images/mobile/object_detection_android_gpu_small.gif\" alt=\"object_detection\"></td>\n<td style=\"text-align:center\"><img src=\"https://google.github.io/mediapipe/images/mobile/object_tracking_android_gpu_small.gif\" alt=\"box_tracking\"></td>\n<td style=\"text-align:center\"><img src=\"https://google.github.io/mediapipe/images/mobile/instant_motion_tracking_android_small.gif\" alt=\"instant_motion_tracking\"></td>\n<td style=\"text-align:center\"><img src=\"https://google.github.io/mediapipe/images/mobile/objectron_chair_android_gpu_small.gif\" alt=\"objectron\"></td>\n<td style=\"text-align:center\"><img src=\"https://google.github.io/mediapipe/images/mobile/template_matching_android_cpu_small.gif\" alt=\"knift\"></td>\n</tr>\n</tbody>\n</table>\n</div>\n<p>所以这个框架很适合二次开发，开发出自己的项目，详细内容就进入官网查看吧。</p>\n<h2 id=\"Hand\"><a href=\"#Hand\" class=\"headerlink\" title=\"Hand\"></a>Hand</h2><p>我自己的一个项目中确实使用到了这个框架，但我只使用过<strong>Hand</strong>项目，接下来我会介绍一下这个项目的使用方法。</p>\n<h3 id=\"介绍\"><a href=\"#介绍\" class=\"headerlink\" title=\"介绍\"></a>介绍</h3><p>这个项目使用了两个网络，一个网络为目标检测，另一个是姿态回归。这个姿态回归的网络还可以直接进行预测是否是“手”，因此虽然有两个网络。</p>\n<p>在视频模式下，在第一次使用目标检测检测到手后，就可以对“手”进行追踪，不断的进行姿态预测，这样可以进一步节省时间。</p>\n<h3 id=\"21个特征点\"><a href=\"#21个特征点\" class=\"headerlink\" title=\"21个特征点\"></a>21个特征点</h3><p>由于是对手进行回归，所以需要了21个手的特征点，如下图所示：</p>\n<p><img src=\"https://google.github.io/mediapipe/images/mobile/hand_landmarks.png\" alt=\"hand_landmarks.png\"></p>\n<p><img src=\"https://google.github.io/mediapipe/images/mobile/hand_crops.png\" alt=\"hand_crops.png\"></p>\n<p>根据自己的需求可以直接查找自己想用的关键点的坐标。</p>\n<h3 id=\"程序\"><a href=\"#程序\" class=\"headerlink\" title=\"程序\"></a>程序</h3><h4 id=\"参数\"><a href=\"#参数\" class=\"headerlink\" title=\"参数\"></a>参数</h4><p><code>Hand</code>提供了三个参数：</p>\n<ul>\n<li><code>STATIC_IMAGE_MODE</code>：<strong>检测模式</strong>，如果是视频检测，设置为<code>False</code>，这样可以追踪，进一步提高速度。</li>\n<li><code>MAX_NUM_HANDS</code>：<strong>手的数量</strong></li>\n<li><code>MIN_DETECTION_CONFIDENCE</code>：<strong>目标检测的置信度</strong></li>\n<li><code>MIN_TRACKING_CONFIDENCE</code>：<strong>姿态检测的置信度</strong></li>\n</ul>\n<h4 id=\"输出\"><a href=\"#输出\" class=\"headerlink\" title=\"输出\"></a>输出</h4><p>结果会得到21个点的坐标，每个点有三个坐标x,y,z的三个坐标。</p>\n<p>没想到吧，这个项目还预测了一部分深度信息，这是最有意思的一个东西。它是根据0号点进行预测，单位是像素。但是深度信息没用，我感觉并不太准。</p>\n<p>同时还可以区分左右手。</p>\n<h4 id=\"程序-1\"><a href=\"#程序-1\" class=\"headerlink\" title=\"程序\"></a>程序</h4><p>我拷贝了官方的程序，这里就不再详细讲了。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> cv2</span><br><span class=\"line\"><span class=\"keyword\">import</span> mediapipe <span class=\"keyword\">as</span> mp</span><br><span class=\"line\">mp_drawing = mp.solutions.drawing_utils</span><br><span class=\"line\">mp_drawing_styles = mp.solutions.drawing_styles</span><br><span class=\"line\">mp_hands = mp.solutions.hands</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># For static images:</span></span><br><span class=\"line\">IMAGE_FILES = []</span><br><span class=\"line\"><span class=\"keyword\">with</span> mp_hands.Hands(</span><br><span class=\"line\">    static_image_mode=<span class=\"literal\">True</span>,</span><br><span class=\"line\">    max_num_hands=<span class=\"number\">2</span>,</span><br><span class=\"line\">    min_detection_confidence=<span class=\"number\">0.5</span>) <span class=\"keyword\">as</span> hands:</span><br><span class=\"line\">  <span class=\"keyword\">for</span> idx, file <span class=\"keyword\">in</span> <span class=\"built_in\">enumerate</span>(IMAGE_FILES):</span><br><span class=\"line\">    <span class=\"comment\"># Read an image, flip it around y-axis for correct handedness output (see</span></span><br><span class=\"line\">    <span class=\"comment\"># above).</span></span><br><span class=\"line\">    image = cv2.flip(cv2.imread(file), <span class=\"number\">1</span>)</span><br><span class=\"line\">    <span class=\"comment\"># Convert the BGR image to RGB before processing.</span></span><br><span class=\"line\">    results = hands.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># Print handedness and draw hand landmarks on the image.</span></span><br><span class=\"line\">    print(<span class=\"string\">&#x27;Handedness:&#x27;</span>, results.multi_handedness)</span><br><span class=\"line\">    <span class=\"keyword\">if</span> <span class=\"keyword\">not</span> results.multi_hand_landmarks:</span><br><span class=\"line\">      <span class=\"keyword\">continue</span></span><br><span class=\"line\">    image_height, image_width, _ = image.shape</span><br><span class=\"line\">    annotated_image = image.copy()</span><br><span class=\"line\">    <span class=\"keyword\">for</span> hand_landmarks <span class=\"keyword\">in</span> results.multi_hand_landmarks:</span><br><span class=\"line\">      print(<span class=\"string\">&#x27;hand_landmarks:&#x27;</span>, hand_landmarks)</span><br><span class=\"line\">      print(</span><br><span class=\"line\">          <span class=\"string\">f&#x27;Index finger tip coordinates: (&#x27;</span>,</span><br><span class=\"line\">          <span class=\"string\">f&#x27;<span class=\"subst\">&#123;hand_landmarks.landmark[mp_hands.HandLandmark.INDEX_FINGER_TIP].x * image_width&#125;</span>, &#x27;</span></span><br><span class=\"line\">          <span class=\"string\">f&#x27;<span class=\"subst\">&#123;hand_landmarks.landmark[mp_hands.HandLandmark.INDEX_FINGER_TIP].y * image_height&#125;</span>)&#x27;</span></span><br><span class=\"line\">      )</span><br><span class=\"line\">      mp_drawing.draw_landmarks(</span><br><span class=\"line\">          annotated_image,</span><br><span class=\"line\">          hand_landmarks,</span><br><span class=\"line\">          mp_hands.HAND_CONNECTIONS,</span><br><span class=\"line\">          mp_drawing_styles.get_default_hand_landmarks_style(),</span><br><span class=\"line\">          mp_drawing_styles.get_default_hand_connections_style())</span><br><span class=\"line\">    cv2.imwrite(</span><br><span class=\"line\">        <span class=\"string\">&#x27;/tmp/annotated_image&#x27;</span> + <span class=\"built_in\">str</span>(idx) + <span class=\"string\">&#x27;.png&#x27;</span>, cv2.flip(annotated_image, <span class=\"number\">1</span>))</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># For webcam input:</span></span><br><span class=\"line\">cap = cv2.VideoCapture(<span class=\"number\">0</span>)</span><br><span class=\"line\"><span class=\"keyword\">with</span> mp_hands.Hands(</span><br><span class=\"line\">    min_detection_confidence=<span class=\"number\">0.5</span>,</span><br><span class=\"line\">    min_tracking_confidence=<span class=\"number\">0.5</span>) <span class=\"keyword\">as</span> hands:</span><br><span class=\"line\">  <span class=\"keyword\">while</span> cap.isOpened():</span><br><span class=\"line\">    success, image = cap.read()</span><br><span class=\"line\">    <span class=\"keyword\">if</span> <span class=\"keyword\">not</span> success:</span><br><span class=\"line\">      print(<span class=\"string\">&quot;Ignoring empty camera frame.&quot;</span>)</span><br><span class=\"line\">      <span class=\"comment\"># If loading a video, use &#x27;break&#x27; instead of &#x27;continue&#x27;.</span></span><br><span class=\"line\">      <span class=\"keyword\">continue</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># To improve performance, optionally mark the image as not writeable to</span></span><br><span class=\"line\">    <span class=\"comment\"># pass by reference.</span></span><br><span class=\"line\">    image.flags.writeable = <span class=\"literal\">False</span></span><br><span class=\"line\">    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)</span><br><span class=\"line\">    results = hands.process(image)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># Draw the hand annotations on the image.</span></span><br><span class=\"line\">    image.flags.writeable = <span class=\"literal\">True</span></span><br><span class=\"line\">    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)</span><br><span class=\"line\">    <span class=\"keyword\">if</span> results.multi_hand_landmarks:</span><br><span class=\"line\">      <span class=\"keyword\">for</span> hand_landmarks <span class=\"keyword\">in</span> results.multi_hand_landmarks:</span><br><span class=\"line\">        mp_drawing.draw_landmarks(</span><br><span class=\"line\">            image,</span><br><span class=\"line\">            hand_landmarks,</span><br><span class=\"line\">            mp_hands.HAND_CONNECTIONS,</span><br><span class=\"line\">            mp_drawing_styles.get_default_hand_landmarks_style(),</span><br><span class=\"line\">            mp_drawing_styles.get_default_hand_connections_style())</span><br><span class=\"line\">    <span class=\"comment\"># Flip the image horizontally for a selfie-view display.</span></span><br><span class=\"line\">    cv2.imshow(<span class=\"string\">&#x27;MediaPipe Hands&#x27;</span>, cv2.flip(image, <span class=\"number\">1</span>))</span><br><span class=\"line\">    <span class=\"keyword\">if</span> cv2.waitKey(<span class=\"number\">5</span>) &amp; <span class=\"number\">0xFF</span> == <span class=\"number\">27</span>:</span><br><span class=\"line\">      <span class=\"keyword\">break</span></span><br><span class=\"line\">cap.release()</span><br></pre></td></tr></table></figure>\n","more":"<p>今天介绍一款Google开源的人工智能项目——Mediapipe。</p>\n<p><img src=\"https://google.github.io/mediapipe/images/mediapipe_small.png\" alt=\"MediaPipe\"></p>\n<p>在这里放出此项目的官网链接：<a href=\"https://google.github.io/mediapipe/\">【点击此处进入官网】</a></p>\n<p>Mediapipe是一个基于图的数据处理管线，用于构建使用了多种形式的数据源，如视频、音频、传感器数据以及任何时间序列数据。</p>\n<p>下面是提供的该项目名单：</p>\n<div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th style=\"text-align:center\"><a href=\"https://google.github.io/mediapipe/solutions/face_detection\"><strong>Face Detection</strong>)</a></th>\n<th style=\"text-align:center\"><a href=\"https://google.github.io/mediapipe/solutions/face_mesh\"><strong>Face Mesh</strong></a></th>\n<th style=\"text-align:center\"><a href=\"https://google.github.io/mediapipe/solutions/iris\"><strong>Iris</strong></a></th>\n<th style=\"text-align:center\"><a href=\"https://google.github.io/mediapipe/solutions/hands\"><strong>Hands</strong></a></th>\n<th style=\"text-align:center\"><a href=\"https://google.github.io/mediapipe/solutions/pose\"><strong>Pose</strong></a></th>\n<th style=\"text-align:center\"><a href=\"https://google.github.io/mediapipe/solutions/holistic\"><strong>Holistic</strong></a></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\"><img src=\"https://google.github.io/mediapipe/images/mobile/face_detection_android_gpu_small.gif\" alt=\"face_detection\"></td>\n<td style=\"text-align:center\"><img src=\"https://google.github.io/mediapipe/images/mobile/face_mesh_android_gpu_small.gif\" alt=\"face_mesh\"></td>\n<td style=\"text-align:center\"><img src=\"https://google.github.io/mediapipe/images/mobile/iris_tracking_android_gpu_small.gif\" alt=\"iris\"></td>\n<td style=\"text-align:center\"><img src=\"https://google.github.io/mediapipe/images/mobile/hand_tracking_android_gpu_small.gif\" alt=\"hand\"></td>\n<td style=\"text-align:center\"><img src=\"https://google.github.io/mediapipe/images/mobile/pose_tracking_android_gpu_small.gif\" alt=\"pose\"></td>\n<td style=\"text-align:center\"><img src=\"https://google.github.io/mediapipe/images/mobile/holistic_tracking_android_gpu_small.gif\" alt=\"hair_segmentation\"></td>\n</tr>\n</tbody>\n</table>\n</div>\n<div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th style=\"text-align:center\"><a href=\"https://google.github.io/mediapipe/solutions/hair_segmentation\"><strong>Hair Segmentation</strong></a></th>\n<th style=\"text-align:center\"><a href=\"https://google.github.io/mediapipe/solutions/object_detection\"><strong>Object Detection</strong></a></th>\n<th style=\"text-align:center\"><a href=\"https://google.github.io/mediapipe/solutions/box_tracking\"><strong>Box Tracking</strong></a></th>\n<th style=\"text-align:center\"><a href=\"https://google.github.io/mediapipe/solutions/instant_motion_tracking\"><strong>Instant Motion Tracking</strong></a></th>\n<th style=\"text-align:center\"><a href=\"https://google.github.io/mediapipe/solutions/objectron\"><strong>Objectron</strong></a></th>\n<th style=\"text-align:center\"><a href=\"https://google.github.io/mediapipe/solutions/knift\"><strong>KNIFT</strong></a></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\"><img src=\"https://google.github.io/mediapipe/images/mobile/hair_segmentation_android_gpu_small.gif\" alt=\"hair_segmentation\"></td>\n<td style=\"text-align:center\"><img src=\"https://google.github.io/mediapipe/images/mobile/object_detection_android_gpu_small.gif\" alt=\"object_detection\"></td>\n<td style=\"text-align:center\"><img src=\"https://google.github.io/mediapipe/images/mobile/object_tracking_android_gpu_small.gif\" alt=\"box_tracking\"></td>\n<td style=\"text-align:center\"><img src=\"https://google.github.io/mediapipe/images/mobile/instant_motion_tracking_android_small.gif\" alt=\"instant_motion_tracking\"></td>\n<td style=\"text-align:center\"><img src=\"https://google.github.io/mediapipe/images/mobile/objectron_chair_android_gpu_small.gif\" alt=\"objectron\"></td>\n<td style=\"text-align:center\"><img src=\"https://google.github.io/mediapipe/images/mobile/template_matching_android_cpu_small.gif\" alt=\"knift\"></td>\n</tr>\n</tbody>\n</table>\n</div>\n<p>所以这个框架很适合二次开发，开发出自己的项目，详细内容就进入官网查看吧。</p>\n<h2 id=\"Hand\"><a href=\"#Hand\" class=\"headerlink\" title=\"Hand\"></a>Hand</h2><p>我自己的一个项目中确实使用到了这个框架，但我只使用过<strong>Hand</strong>项目，接下来我会介绍一下这个项目的使用方法。</p>\n<h3 id=\"介绍\"><a href=\"#介绍\" class=\"headerlink\" title=\"介绍\"></a>介绍</h3><p>这个项目使用了两个网络，一个网络为目标检测，另一个是姿态回归。这个姿态回归的网络还可以直接进行预测是否是“手”，因此虽然有两个网络。</p>\n<p>在视频模式下，在第一次使用目标检测检测到手后，就可以对“手”进行追踪，不断的进行姿态预测，这样可以进一步节省时间。</p>\n<h3 id=\"21个特征点\"><a href=\"#21个特征点\" class=\"headerlink\" title=\"21个特征点\"></a>21个特征点</h3><p>由于是对手进行回归，所以需要了21个手的特征点，如下图所示：</p>\n<p><img src=\"https://google.github.io/mediapipe/images/mobile/hand_landmarks.png\" alt=\"hand_landmarks.png\"></p>\n<p><img src=\"https://google.github.io/mediapipe/images/mobile/hand_crops.png\" alt=\"hand_crops.png\"></p>\n<p>根据自己的需求可以直接查找自己想用的关键点的坐标。</p>\n<h3 id=\"程序\"><a href=\"#程序\" class=\"headerlink\" title=\"程序\"></a>程序</h3><h4 id=\"参数\"><a href=\"#参数\" class=\"headerlink\" title=\"参数\"></a>参数</h4><p><code>Hand</code>提供了三个参数：</p>\n<ul>\n<li><code>STATIC_IMAGE_MODE</code>：<strong>检测模式</strong>，如果是视频检测，设置为<code>False</code>，这样可以追踪，进一步提高速度。</li>\n<li><code>MAX_NUM_HANDS</code>：<strong>手的数量</strong></li>\n<li><code>MIN_DETECTION_CONFIDENCE</code>：<strong>目标检测的置信度</strong></li>\n<li><code>MIN_TRACKING_CONFIDENCE</code>：<strong>姿态检测的置信度</strong></li>\n</ul>\n<h4 id=\"输出\"><a href=\"#输出\" class=\"headerlink\" title=\"输出\"></a>输出</h4><p>结果会得到21个点的坐标，每个点有三个坐标x,y,z的三个坐标。</p>\n<p>没想到吧，这个项目还预测了一部分深度信息，这是最有意思的一个东西。它是根据0号点进行预测，单位是像素。但是深度信息没用，我感觉并不太准。</p>\n<p>同时还可以区分左右手。</p>\n<h4 id=\"程序-1\"><a href=\"#程序-1\" class=\"headerlink\" title=\"程序\"></a>程序</h4><p>我拷贝了官方的程序，这里就不再详细讲了。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> cv2</span><br><span class=\"line\"><span class=\"keyword\">import</span> mediapipe <span class=\"keyword\">as</span> mp</span><br><span class=\"line\">mp_drawing = mp.solutions.drawing_utils</span><br><span class=\"line\">mp_drawing_styles = mp.solutions.drawing_styles</span><br><span class=\"line\">mp_hands = mp.solutions.hands</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># For static images:</span></span><br><span class=\"line\">IMAGE_FILES = []</span><br><span class=\"line\"><span class=\"keyword\">with</span> mp_hands.Hands(</span><br><span class=\"line\">    static_image_mode=<span class=\"literal\">True</span>,</span><br><span class=\"line\">    max_num_hands=<span class=\"number\">2</span>,</span><br><span class=\"line\">    min_detection_confidence=<span class=\"number\">0.5</span>) <span class=\"keyword\">as</span> hands:</span><br><span class=\"line\">  <span class=\"keyword\">for</span> idx, file <span class=\"keyword\">in</span> <span class=\"built_in\">enumerate</span>(IMAGE_FILES):</span><br><span class=\"line\">    <span class=\"comment\"># Read an image, flip it around y-axis for correct handedness output (see</span></span><br><span class=\"line\">    <span class=\"comment\"># above).</span></span><br><span class=\"line\">    image = cv2.flip(cv2.imread(file), <span class=\"number\">1</span>)</span><br><span class=\"line\">    <span class=\"comment\"># Convert the BGR image to RGB before processing.</span></span><br><span class=\"line\">    results = hands.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># Print handedness and draw hand landmarks on the image.</span></span><br><span class=\"line\">    print(<span class=\"string\">&#x27;Handedness:&#x27;</span>, results.multi_handedness)</span><br><span class=\"line\">    <span class=\"keyword\">if</span> <span class=\"keyword\">not</span> results.multi_hand_landmarks:</span><br><span class=\"line\">      <span class=\"keyword\">continue</span></span><br><span class=\"line\">    image_height, image_width, _ = image.shape</span><br><span class=\"line\">    annotated_image = image.copy()</span><br><span class=\"line\">    <span class=\"keyword\">for</span> hand_landmarks <span class=\"keyword\">in</span> results.multi_hand_landmarks:</span><br><span class=\"line\">      print(<span class=\"string\">&#x27;hand_landmarks:&#x27;</span>, hand_landmarks)</span><br><span class=\"line\">      print(</span><br><span class=\"line\">          <span class=\"string\">f&#x27;Index finger tip coordinates: (&#x27;</span>,</span><br><span class=\"line\">          <span class=\"string\">f&#x27;<span class=\"subst\">&#123;hand_landmarks.landmark[mp_hands.HandLandmark.INDEX_FINGER_TIP].x * image_width&#125;</span>, &#x27;</span></span><br><span class=\"line\">          <span class=\"string\">f&#x27;<span class=\"subst\">&#123;hand_landmarks.landmark[mp_hands.HandLandmark.INDEX_FINGER_TIP].y * image_height&#125;</span>)&#x27;</span></span><br><span class=\"line\">      )</span><br><span class=\"line\">      mp_drawing.draw_landmarks(</span><br><span class=\"line\">          annotated_image,</span><br><span class=\"line\">          hand_landmarks,</span><br><span class=\"line\">          mp_hands.HAND_CONNECTIONS,</span><br><span class=\"line\">          mp_drawing_styles.get_default_hand_landmarks_style(),</span><br><span class=\"line\">          mp_drawing_styles.get_default_hand_connections_style())</span><br><span class=\"line\">    cv2.imwrite(</span><br><span class=\"line\">        <span class=\"string\">&#x27;/tmp/annotated_image&#x27;</span> + <span class=\"built_in\">str</span>(idx) + <span class=\"string\">&#x27;.png&#x27;</span>, cv2.flip(annotated_image, <span class=\"number\">1</span>))</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># For webcam input:</span></span><br><span class=\"line\">cap = cv2.VideoCapture(<span class=\"number\">0</span>)</span><br><span class=\"line\"><span class=\"keyword\">with</span> mp_hands.Hands(</span><br><span class=\"line\">    min_detection_confidence=<span class=\"number\">0.5</span>,</span><br><span class=\"line\">    min_tracking_confidence=<span class=\"number\">0.5</span>) <span class=\"keyword\">as</span> hands:</span><br><span class=\"line\">  <span class=\"keyword\">while</span> cap.isOpened():</span><br><span class=\"line\">    success, image = cap.read()</span><br><span class=\"line\">    <span class=\"keyword\">if</span> <span class=\"keyword\">not</span> success:</span><br><span class=\"line\">      print(<span class=\"string\">&quot;Ignoring empty camera frame.&quot;</span>)</span><br><span class=\"line\">      <span class=\"comment\"># If loading a video, use &#x27;break&#x27; instead of &#x27;continue&#x27;.</span></span><br><span class=\"line\">      <span class=\"keyword\">continue</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># To improve performance, optionally mark the image as not writeable to</span></span><br><span class=\"line\">    <span class=\"comment\"># pass by reference.</span></span><br><span class=\"line\">    image.flags.writeable = <span class=\"literal\">False</span></span><br><span class=\"line\">    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)</span><br><span class=\"line\">    results = hands.process(image)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># Draw the hand annotations on the image.</span></span><br><span class=\"line\">    image.flags.writeable = <span class=\"literal\">True</span></span><br><span class=\"line\">    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)</span><br><span class=\"line\">    <span class=\"keyword\">if</span> results.multi_hand_landmarks:</span><br><span class=\"line\">      <span class=\"keyword\">for</span> hand_landmarks <span class=\"keyword\">in</span> results.multi_hand_landmarks:</span><br><span class=\"line\">        mp_drawing.draw_landmarks(</span><br><span class=\"line\">            image,</span><br><span class=\"line\">            hand_landmarks,</span><br><span class=\"line\">            mp_hands.HAND_CONNECTIONS,</span><br><span class=\"line\">            mp_drawing_styles.get_default_hand_landmarks_style(),</span><br><span class=\"line\">            mp_drawing_styles.get_default_hand_connections_style())</span><br><span class=\"line\">    <span class=\"comment\"># Flip the image horizontally for a selfie-view display.</span></span><br><span class=\"line\">    cv2.imshow(<span class=\"string\">&#x27;MediaPipe Hands&#x27;</span>, cv2.flip(image, <span class=\"number\">1</span>))</span><br><span class=\"line\">    <span class=\"keyword\">if</span> cv2.waitKey(<span class=\"number\">5</span>) &amp; <span class=\"number\">0xFF</span> == <span class=\"number\">27</span>:</span><br><span class=\"line\">      <span class=\"keyword\">break</span></span><br><span class=\"line\">cap.release()</span><br></pre></td></tr></table></figure>\n","categories":[{"name":"人工智能","path":"api/categories/人工智能.json"}],"tags":[{"name":"python","path":"api/tags/python.json"},{"name":"深度学习","path":"api/tags/深度学习.json"}]}