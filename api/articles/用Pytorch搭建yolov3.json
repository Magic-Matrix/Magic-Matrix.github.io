{"title":"用Pytorch搭建yolov3","slug":"用Pytorch搭建yolov3","date":"2021-02-06T06:13:36.000Z","updated":"2021-10-14T06:58:49.576Z","comments":true,"path":"api/articles/用Pytorch搭建yolov3.json","excerpt":null,"covers":["/2021/02/%E7%94%A8Pytorch%E6%90%AD%E5%BB%BAyolov3/20210207195622.jpg","/2021/02/%E7%94%A8Pytorch%E6%90%AD%E5%BB%BAyolov3/20191020111518954.jpg","/2021/02/%E7%94%A8Pytorch%E6%90%AD%E5%BB%BAyolov3/2019.jpg","/2021/02/%E7%94%A8Pytorch%E6%90%AD%E5%BB%BAyolov3/1437686-20200102124439991-778619867.png","/2021/02/%E7%94%A8Pytorch%E6%90%AD%E5%BB%BAyolov3/1437686-20191230192708001-430020660.png","/2021/02/%E7%94%A8Pytorch%E6%90%AD%E5%BB%BAyolov3/10201.jpg","/2021/02/%E7%94%A8Pytorch%E6%90%AD%E5%BB%BAyolov3/0111.jpg","/2021/02/%E7%94%A8Pytorch%E6%90%AD%E5%BB%BAyolov3/1437686-20200102121153977-382469172.png","/2021/02/%E7%94%A8Pytorch%E6%90%AD%E5%BB%BAyolov3/20190509224534499.png","/2021/02/%E7%94%A8Pytorch%E6%90%AD%E5%BB%BAyolov3/8694a4c27d1ed21bc6ed15f6c38952c250da3fee.jpeg"],"content":"<p>YOLO是“You Only Look Once”的缩写，YOLO将物体检测作为回归问题求解，是一个对象检测算法的名字，这是Redmon等人在2016年的一篇研究论文中命名的。</p>\n<p><img src=\"/2021/02/%E7%94%A8Pytorch%E6%90%AD%E5%BB%BAyolov3/20210207195622.jpg\" alt=\"20191020111518954\" style=\"zoom:50%;\"></p>\n<p>本篇文章会一步一步搭建起yolov3的神经网络，详细内容会具体分析。</p>\n<h2 id=\"YOLO介绍\"><a href=\"#YOLO介绍\" class=\"headerlink\" title=\"YOLO介绍\"></a>YOLO介绍</h2><p>目标检测（object detection）是一个因近年来深度学习的发展而受益颇多的领域，近年来，人们开发了多种目标检测算法，其中包括YOLO、SSD、Mask-RCNN和RetinaNet。此篇文章使用PyTorch并基于YOLO v3来实现一个目标检测器，这是一种速度更快的目标检测算法。</p>\n<p>YOLO是“You Only Look Once”的缩写，YOLO将物体检测作为回归问题求解，是一个对象检测算法的名字，这是Redmon等人在2016年的一篇研究论文中命名的：</p>\n<blockquote>\n<p><strong><em>Redmon J , Divvala S , Girshick R , et al. You Only Look Once: Unified, Real-Time Object Detection[J]. 2015.</em></strong></p>\n</blockquote>\n<h2 id=\"YOLO结构\"><a href=\"#YOLO结构\" class=\"headerlink\" title=\"YOLO结构\"></a>YOLO结构</h2><p>以下为yolo的整体结构：</p>\n<p><img src=\"/2021/02/%E7%94%A8Pytorch%E6%90%AD%E5%BB%BAyolov3/20191020111518954.jpg\" alt=\"20191020111518954\"></p>\n<p>这张图分成两部分，左边虚线框起来的是<strong>主干特征提取</strong>部分。</p>\n<h3 id=\"主干特征提取\"><a href=\"#主干特征提取\" class=\"headerlink\" title=\"主干特征提取\"></a>主干特征提取</h3><p>这部分称作Darknet-53，主干特征提取顾名思义，用来提取图片的特征。</p>\n<p>输入是需要一张$416 <em> 416 </em> 3$大小的图片，然后是不断卷积的过程，如果仔细看会发现图片的高和宽不断被压缩，通道数却不断扩张，这是一个<strong>下采样</strong>过程。</p>\n<p>经过下采样之后，会获得<strong>特征层</strong>（用来表述图片的特征），下图是我们需要的特征层。</p>\n<p><img src=\"/2021/02/%E7%94%A8Pytorch%E6%90%AD%E5%BB%BAyolov3/2019.jpg\" alt=\"2019\"></p>\n<p>由此可见保留下来的特征层为：$52<em>52</em>256$、$26<em>26</em>512$、$13<em>13</em>1024$三种尺寸，这三种特征层是即将传入右边的部分。</p>\n<p><img src=\"/2021/02/%E7%94%A8Pytorch%E6%90%AD%E5%BB%BAyolov3/1437686-20200102124439991-778619867.png\" alt=\"1437686-20200102124439991-778619867\"></p>\n<p>每个特征图可以看作一个“条目”，因为一个“条目”有一些信息，如下图所示：</p>\n<p><img src=\"/2021/02/%E7%94%A8Pytorch%E6%90%AD%E5%BB%BAyolov3/1437686-20191230192708001-430020660.png\" alt=\"1437686-20191230192708001-430020660\"></p>\n<h3 id=\"其他部分\"><a href=\"#其他部分\" class=\"headerlink\" title=\"其他部分\"></a>其他部分</h3><p>这里的其他部分泛指右边部分的网络。</p>\n<p>经过主干特征提取后，我们获得了三种尺寸的特征层，分别对三种尺寸的特征层处理。</p>\n<h4 id=\"处理-13131024-特征层\"><a href=\"#处理-13131024-特征层\" class=\"headerlink\" title=\"处理$13131024$特征层\"></a>处理$13<em>13</em>1024$特征层</h4><p>首先说一下$13<em>13</em>1024$特征层的处理：</p>\n<p><img src=\"/2021/02/%E7%94%A8Pytorch%E6%90%AD%E5%BB%BAyolov3/10201.jpg\" alt=\"10201\"></p>\n<p>可以看到$13<em>13</em>1024$特征层经过了5次卷积后传到了两个方向。</p>\n<p>右边的粉色部分是<strong>分类预测</strong>和<strong>回归预测</strong>，其实就是两次卷积，最后会获得$13<em>13</em>75$大小的，再经过一个分解变成$13<em>13</em>3<em>25$，即$13</em>13<em>3</em>(20 + 1 + 4)$，这个过程其实就是化成$13<em>13$的网格，每个网格有<strong>3个</strong>预测出的<em>*先验框</em></em>，接下来会根据判断属于那种框的尺寸。</p>\n<p>之所以会把25分成三部分，其实就gailv是20个物体分类，由于使用<strong>voc数据集</strong>，所以会有20个，<strong>coco训练集</strong>会出现80个，简单来讲20个<strong>置信度（属于哪个类的概率）</strong>会分类属于哪个类；1是指是否有物体；4是对框的调整。</p>\n<p>注意$13<em>13</em>1024$特征层还有一个传递方向，这时需要一个<strong>上采样</strong>，也就是扩增长宽，减小通道数。经过上采样之后会和$26<em>26</em>512$特征层进行<strong>堆叠（增加通道数）</strong>。</p>\n<h4 id=\"处理-2626512-特征层\"><a href=\"#处理-2626512-特征层\" class=\"headerlink\" title=\"处理$2626512$特征层\"></a>处理$26<em>26</em>512$特征层</h4><p>$26<em>26</em>512$特征层在和经过上采样的$13<em>13</em>1024$特征层进行堆叠后，会形成一个新的“特征金字塔”，当然这个特征金字塔还会继续堆叠。</p>\n<p><img src=\"/2021/02/%E7%94%A8Pytorch%E6%90%AD%E5%BB%BAyolov3/0111.jpg\" alt=\"0111\"></p>\n<p>后面的部分就一样了，再5次卷积，在最后进行分类预测和回归预测，会出现相同操作。</p>\n<p>与此同时会把5次卷积后继续向上传递，并有个上采样。</p>\n<p>$52<em>52</em>256$特征层同样如此就不再赘述。</p>\n<p>输出结果：</p>\n<p>$t_x$和$t_y$是被检测物体的中心位置，$t_w$和$t_h$是方框的尺寸，$p_0$是<strong>物体置信度</strong>，代表了这个区域内有物体的概率，$p_1-&gt;p_c$是分类置信度，哪类概率高，就属于什么物体。最后还有个B，这个是锚框个数，说明了这个区域可以最多检测出B个物体。</p>\n<h3 id=\"Anchor-Boxes\"><a href=\"#Anchor-Boxes\" class=\"headerlink\" title=\"Anchor Boxes\"></a>Anchor Boxes</h3><p>有的翻译成锚框，有的翻译成先验框，这里我就叫它锚框了。</p>\n<p>YOLO不能直接预测边界框的宽度和高度，这会导致训练过程中出现不稳定的变化。大多数现代目标检测器会预测对数空间转换，或者只是偏移到称为锚点的预定义默认锚框。YOLO-v3具有三个锚点，可在每个细胞单元格上预测三个边界框。</p>\n<p>那么真正的方框是怎么预测出的？先看下面的公式：</p>\n<script type=\"math/tex; mode=display\">\nb_x = \\sigma(t_x) + c_x\\\\\nb_y = \\sigma(t_y) + c_y\\\\\nb_w = p_w e^{t_w}\\\\\nb_h = p_h e^{t_h}</script><p>前两个公式是预测中心点，经过$\\sigma()$函数后就会稳定在0和1之间。$c_x$和$c_y$是第几个方框，例如上图中红色框的这两值都是6。</p>\n<p>后面两个公式预测宽度，$p_w$和$p_h$是锚框的尺寸，直接乘就可以。</p>\n<p>还没有完，这只完成了一半，接着看：</p>\n<p><img src=\"/2021/02/%E7%94%A8Pytorch%E6%90%AD%E5%BB%BAyolov3/1437686-20200102121153977-382469172.png\" alt=\"1437686-20200102121153977-382469172\"></p>\n<p>中心点需要还乘上对应的网格宽度，方框需要带入e指数中。</p>\n<p>补一张更漂亮的图。</p>\n<p><img src=\"/2021/02/%E7%94%A8Pytorch%E6%90%AD%E5%BB%BAyolov3/20190509224534499.png\" alt=\"v2-fb8b964727ccfea93345ba1361c4c8a3_720w\"></p>\n<h2 id=\"YOLO预测原理\"><a href=\"#YOLO预测原理\" class=\"headerlink\" title=\"YOLO预测原理\"></a>YOLO预测原理</h2><p>YOLO将图片分成了3种检测，检测的区别是按照分割区域的大小。</p>\n<p>首先输入图片的大小要定下为$416*416$大小，如果不合适就要补上缺失部分，目的就是防止失真。</p>\n<p>将图片分别分成$52<em>52$、$26</em>26$、$13*13$、三种尺寸的网格，针对的识别三种尺寸，也就对应上了上部分的三种输出结果。</p>\n<p>每个输出结果的维度为<code>(N, 75 * 3, x, x)</code>，x为尺寸分别对应52、26、13三种，N代表样本数，75 * 3这个数在上面提过了就不再多说。</p>\n<p>最后的最终的预测结构后还要进行得分排序与非极大抑制筛选。</p>\n<p>用$7<em>7$来举例（我只找到$7</em>7$的例子，我又懒得去做$13<em>13$的图），下面将一幅图分成$7</em>7$网格，共49部分。</p>\n<p><img src=\"/2021/02/%E7%94%A8Pytorch%E6%90%AD%E5%BB%BAyolov3/8694a4c27d1ed21bc6ed15f6c38952c250da3fee.jpeg\" alt=\"8694a4c27d1ed21bc6ed15f6c38952c250da3fee\"></p>\n<p>对于每个网格点，都会预测一个边界框和与每个类别（汽车，行人，交通信号灯等）相对应的概率，每个网络点负责一个区域的检测。</p>\n<h2 id=\"程序\"><a href=\"#程序\" class=\"headerlink\" title=\"程序\"></a>程序</h2><p>首先搭建DarkNet大概框架：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">DarkNet</span>(<span class=\"params\">nn.Module</span>):</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span>(<span class=\"params\">self, layers</span>):</span></span><br><span class=\"line\">        <span class=\"built_in\">super</span>(DarkNet, self).__init__()</span><br></pre></td></tr></table></figure>\n<p>DarkNet继承了pytorch中的模型，目的使用一些相同的框架函数。</p>\n<h3 id=\"初始的卷积层\"><a href=\"#初始的卷积层\" class=\"headerlink\" title=\"初始的卷积层\"></a>初始的卷积层</h3><p>继续根据整体结构来加入每一层，首先卷积层：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 设置卷积核个数，也是卷积后的通道数</span></span><br><span class=\"line\">self.inplanes = <span class=\"number\">32</span></span><br><span class=\"line\"><span class=\"comment\"># 大概配置参数是：</span></span><br><span class=\"line\"><span class=\"comment\"># 输入通道数、输出通道数（卷积核个数）、卷积核尺寸、步长填充、是否加入偏置</span></span><br><span class=\"line\"><span class=\"comment\"># 此时下面的配置是same卷积，没有破坏图像原本的大小</span></span><br><span class=\"line\">self.conv1 = nn.Conv2d(<span class=\"number\">3</span>, self.inplanes, kernel_size=<span class=\"number\">3</span>, stride=<span class=\"number\">1</span>, padding=<span class=\"number\">1</span>, bias=<span class=\"literal\">False</span>)</span><br></pre></td></tr></table></figure>\n<p>标准化和激活函数：</p>\n<p><strong>注意</strong>：这里使用的是LeakyReLU，这样好处是，负数区域还存在值，同时也有斜率。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># BatchNorm标准化，加速收敛速度及稳定性的算法</span></span><br><span class=\"line\">self.bn1 = nn.BatchNorm2d(self.inplanes)</span><br><span class=\"line\"><span class=\"comment\"># 设置激活函数</span></span><br><span class=\"line\">self.relu1 = nn.LeakyReLU(<span class=\"number\">0.1</span>)</span><br></pre></td></tr></table></figure>\n<p>以上操作建立了一个卷积层，最后生成的维度为：<code>(N, 416, 416, 32)</code></p>\n<h3 id=\"残差块\"><a href=\"#残差块\" class=\"headerlink\" title=\"残差块\"></a>残差块</h3><p>先制做出残差块的函数，返回值是一个残差块：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">_make_layer</span>(<span class=\"params\">self, planes, blocks</span>):</span></span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">    制作一个结构块</span></span><br><span class=\"line\"><span class=\"string\">    :param planes: 是个列表，第一位置是输入通道数，第二位置是输出通道数</span></span><br><span class=\"line\"><span class=\"string\">    :param blocks: 残差块堆叠个数</span></span><br><span class=\"line\"><span class=\"string\">    :return:</span></span><br><span class=\"line\"><span class=\"string\">    &quot;&quot;&quot;</span></span><br><span class=\"line\">    layers = []</span><br><span class=\"line\">    <span class=\"comment\"># 下采样，步长为2，卷积核大小为3，填充为1</span></span><br><span class=\"line\">    <span class=\"comment\"># 这样长宽就可以压缩</span></span><br><span class=\"line\">    layers.append((<span class=\"string\">&quot;ds_conv&quot;</span>, nn.Conv2d(self.inplanes, planes[<span class=\"number\">1</span>], kernel_size=<span class=\"number\">3</span>,</span><br><span class=\"line\">                            stride=<span class=\"number\">2</span>, padding=<span class=\"number\">1</span>, bias=<span class=\"literal\">False</span>)))</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># BatchNorm标准化</span></span><br><span class=\"line\">    layers.append((<span class=\"string\">&quot;ds_bn&quot;</span>, nn.BatchNorm2d(planes[<span class=\"number\">1</span>])))</span><br><span class=\"line\">    <span class=\"comment\"># 设置激活函数</span></span><br><span class=\"line\">    layers.append((<span class=\"string\">&quot;ds_relu&quot;</span>, nn.LeakyReLU(<span class=\"number\">0.1</span>)))</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># 加入darknet模块</span></span><br><span class=\"line\">    self.inplanes = planes[<span class=\"number\">1</span>]</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># 堆叠残差块</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"number\">0</span>, blocks):</span><br><span class=\"line\">        <span class=\"comment\"># 添加一个网络</span></span><br><span class=\"line\">        <span class=\"comment\"># layers是个列表，里面元素是元组，元组元素是字符串和BasicBlock网络块</span></span><br><span class=\"line\">        layers.append((<span class=\"string\">&quot;residual_&#123;&#125;&quot;</span>.<span class=\"built_in\">format</span>(i), BasicBlock(self.inplanes, planes)))</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># 打包网络</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> nn.Sequential(OrderedDict(layers))</span><br></pre></td></tr></table></figure>\n<p>OrderedDict是有序字典，虽然平常不常用，但在这里使用是最合适的，记得导入包：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> collections <span class=\"keyword\">import</span> OrderedDict</span><br></pre></td></tr></table></figure>\n<p>最核心的部分是BasicBlock类，这里是残差网络相加的地方：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 基本的darknet块（残差块的核心）</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">BasicBlock</span>(<span class=\"params\">nn.Module</span>):</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span>(<span class=\"params\">self, inplanes, planes</span>):</span></span><br><span class=\"line\">        <span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">        残差块</span></span><br><span class=\"line\"><span class=\"string\">        :param inplanes:</span></span><br><span class=\"line\"><span class=\"string\">        :param planes: 下采样部分</span></span><br><span class=\"line\"><span class=\"string\">        &quot;&quot;&quot;</span></span><br><span class=\"line\">        <span class=\"built_in\">super</span>(BasicBlock, self).__init__()</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># 定义了两组卷积操作</span></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># 最开始是卷积核形状为1的卷积</span></span><br><span class=\"line\">        <span class=\"comment\"># 目的是下降通道数</span></span><br><span class=\"line\">        self.conv1 = nn.Conv2d(inplanes, planes[<span class=\"number\">0</span>], kernel_size=<span class=\"number\">1</span>,</span><br><span class=\"line\">                               stride=<span class=\"number\">1</span>, padding=<span class=\"number\">0</span>, bias=<span class=\"literal\">False</span>)</span><br><span class=\"line\">        <span class=\"comment\"># BatchNorm标准化</span></span><br><span class=\"line\">        self.bn1 = nn.BatchNorm2d(planes[<span class=\"number\">0</span>])</span><br><span class=\"line\">        <span class=\"comment\"># 设置激活函数</span></span><br><span class=\"line\">        self.relu1 = nn.LeakyReLU(<span class=\"number\">0.1</span>)</span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"comment\"># 为了保证残差成功，必须使用same卷积</span></span><br><span class=\"line\">        <span class=\"comment\"># 这里还将通道数提升了</span></span><br><span class=\"line\">        self.conv2 = nn.Conv2d(planes[<span class=\"number\">0</span>], planes[<span class=\"number\">1</span>], kernel_size=<span class=\"number\">3</span>,</span><br><span class=\"line\">                               stride=<span class=\"number\">1</span>, padding=<span class=\"number\">1</span>, bias=<span class=\"literal\">False</span>)</span><br><span class=\"line\">        <span class=\"comment\"># BatchNorm标准化</span></span><br><span class=\"line\">        self.bn2 = nn.BatchNorm2d(planes[<span class=\"number\">1</span>])</span><br><span class=\"line\">        <span class=\"comment\"># 设置激活函数</span></span><br><span class=\"line\">        self.relu2 = nn.LeakyReLU(<span class=\"number\">0.1</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">forward</span>(<span class=\"params\">self, x</span>):</span></span><br><span class=\"line\">        <span class=\"comment\"># 保存残差”边“</span></span><br><span class=\"line\">        residual = x</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># 传统艺能</span></span><br><span class=\"line\">        </span><br><span class=\"line\">        out = self.conv1(x)</span><br><span class=\"line\">        out = self.bn1(out)</span><br><span class=\"line\">        out = self.relu1(out)</span><br><span class=\"line\"></span><br><span class=\"line\">        out = self.conv2(out)</span><br><span class=\"line\">        out = self.bn2(out)</span><br><span class=\"line\">        out = self.relu2(out)</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># 网络最后加上残差边</span></span><br><span class=\"line\">        out += residual</span><br><span class=\"line\">        <span class=\"keyword\">return</span> out</span><br></pre></td></tr></table></figure>\n<p>由于里面先进行1卷积，再3卷积，这样可以减少参数量，</p>\n<p>初始化剩下的东西：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">   <span class=\"comment\"># </span></span><br><span class=\"line\">self.layers_out_filters = [<span class=\"number\">64</span>, <span class=\"number\">128</span>, <span class=\"number\">256</span>, <span class=\"number\">512</span>, <span class=\"number\">1024</span>]</span><br><span class=\"line\"></span><br><span class=\"line\">   <span class=\"comment\"># 进行权值初始化</span></span><br><span class=\"line\">   <span class=\"keyword\">for</span> m <span class=\"keyword\">in</span> self.modules():</span><br><span class=\"line\">       <span class=\"keyword\">if</span> <span class=\"built_in\">isinstance</span>(m, nn.Conv2d):</span><br><span class=\"line\">           n = m.kernel_size[<span class=\"number\">0</span>] * m.kernel_size[<span class=\"number\">1</span>] * m.out_channels</span><br><span class=\"line\">           m.weight.data.normal_(<span class=\"number\">0</span>, math.sqrt(<span class=\"number\">2.</span> / n))</span><br><span class=\"line\">       <span class=\"keyword\">elif</span> <span class=\"built_in\">isinstance</span>(m, nn.BatchNorm2d):</span><br><span class=\"line\">           m.weight.data.fill_(<span class=\"number\">1</span>)</span><br><span class=\"line\">           m.bias.data.zero_()</span><br></pre></td></tr></table></figure>\n<h3 id=\"前向传播\"><a href=\"#前向传播\" class=\"headerlink\" title=\"前向传播\"></a>前向传播</h3><p>前向传播的时候我们需要返回三个尺寸的特征图，所以需要这样写：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">forward</span>(<span class=\"params\">self, x</span>):</span></span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">    前向传播</span></span><br><span class=\"line\"><span class=\"string\">    :param x: 输入图</span></span><br><span class=\"line\"><span class=\"string\">    :return: 三个特征图</span></span><br><span class=\"line\"><span class=\"string\">    &quot;&quot;&quot;</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># 传统艺能</span></span><br><span class=\"line\">    x = self.conv1(x)</span><br><span class=\"line\">    x = self.bn1(x)</span><br><span class=\"line\">    x = self.relu1(x)</span><br><span class=\"line\"></span><br><span class=\"line\">    x = self.layer1(x)</span><br><span class=\"line\">    x = self.layer2(x)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># 第一特征图</span></span><br><span class=\"line\">    out3 = self.layer3(x)</span><br><span class=\"line\">    <span class=\"comment\"># 第二特征图</span></span><br><span class=\"line\">    out4 = self.layer4(out3)</span><br><span class=\"line\">    <span class=\"comment\"># 第三特征图</span></span><br><span class=\"line\">    out5 = self.layer5(out4)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">return</span> out3, out4, out5</span><br></pre></td></tr></table></figure>\n<h3 id=\"从特征获取预测结果\"><a href=\"#从特征获取预测结果\" class=\"headerlink\" title=\"从特征获取预测结果\"></a>从特征获取预测结果</h3><p>特征图出来了，但我们需要将特征图转换成最终结果。</p>\n<p>在放代码之前，自定义一套卷积，这是方便后面快速使用：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">conv2d</span>(<span class=\"params\">filter_in, filter_out, kernel_size</span>):</span></span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">    私自定义的卷积</span></span><br><span class=\"line\"><span class=\"string\">    :param filter_in: 输入通道数</span></span><br><span class=\"line\"><span class=\"string\">    :param filter_out: 输出通道数</span></span><br><span class=\"line\"><span class=\"string\">    :param kernel_size: 卷积核尺寸</span></span><br><span class=\"line\"><span class=\"string\">    :return:</span></span><br><span class=\"line\"><span class=\"string\">    &quot;&quot;&quot;</span></span><br><span class=\"line\">    pad = (kernel_size - <span class=\"number\">1</span>) // <span class=\"number\">2</span> <span class=\"keyword\">if</span> kernel_size <span class=\"keyword\">else</span> <span class=\"number\">0</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> nn.Sequential(OrderedDict([</span><br><span class=\"line\">        (<span class=\"string\">&quot;conv&quot;</span>, nn.Conv2d(filter_in, filter_out, kernel_size=kernel_size, stride=<span class=\"number\">1</span>, padding=pad, bias=<span class=\"literal\">False</span>)),</span><br><span class=\"line\">        (<span class=\"string\">&quot;bn&quot;</span>, nn.BatchNorm2d(filter_out)),</span><br><span class=\"line\">        (<span class=\"string\">&quot;relu&quot;</span>, nn.LeakyReLU(<span class=\"number\">0.1</span>)),</span><br><span class=\"line\">    ]))</span><br></pre></td></tr></table></figure>\n<p>这个卷积相当于一套完整的卷积层，但官方这里没有使用残差网络，这有点让我感到疑惑，因为我担心这样会不会影响整个网络前部分的梯度计算。</p>\n<h4 id=\"主体部分的初始化\"><a href=\"#主体部分的初始化\" class=\"headerlink\" title=\"主体部分的初始化\"></a>主体部分的初始化</h4><p>因为是主体部分，所以首先创建yolo主体：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">YoloBody</span>(<span class=\"params\">nn.Module</span>):</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span>(<span class=\"params\">self, config</span>):</span></span><br><span class=\"line\">        <span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">        yolo主体</span></span><br><span class=\"line\"><span class=\"string\">        :param config:</span></span><br><span class=\"line\"><span class=\"string\">        &quot;&quot;&quot;</span></span><br><span class=\"line\">        <span class=\"built_in\">super</span>(YoloBody, self).__init__()</span><br><span class=\"line\">        self.config = config</span><br></pre></td></tr></table></figure>\n<p>然后获取已经创建好的网络（也就是上面的程序）：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 获取darknet53堆叠网络</span></span><br><span class=\"line\">self.backbone = darknet53(<span class=\"literal\">None</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 获取输出通道数</span></span><br><span class=\"line\">out_filters = self.backbone.layers_out_filters</span><br></pre></td></tr></table></figure>\n<p>接下来就是$13*13$特征层的提取：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># last_layer0</span></span><br><span class=\"line\"><span class=\"comment\"># 此值为75</span></span><br><span class=\"line\">final_out_filter0 = <span class=\"built_in\">len</span>(config[<span class=\"string\">&quot;yolo&quot;</span>][<span class=\"string\">&quot;anchors&quot;</span>][<span class=\"number\">0</span>]) * (<span class=\"number\">5</span> + config[<span class=\"string\">&quot;yolo&quot;</span>][<span class=\"string\">&quot;classes&quot;</span>])</span><br><span class=\"line\"><span class=\"comment\"># 5次卷积+2次卷积</span></span><br><span class=\"line\">self.last_layer0 = make_last_layers([<span class=\"number\">512</span>, <span class=\"number\">1024</span>], out_filters[-<span class=\"number\">1</span>], final_out_filter0)</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<p>现在详细说一下<code>final_out_filter0</code>这个变量特点，此变量分两部分，两部分相乘才出结果。<code>len(config[&quot;yolo&quot;][&quot;anchors&quot;][0])</code>是<strong>先验框</strong>个数，我们使用了3个，所以此结果是3；<code>(5 + config[&quot;yolo&quot;][&quot;classes&quot;])</code>这个是$20 + 1 + 4$，也就是25。最终结果就是$(20+1+4)*3=75$。</p>\n<p>这个时候有个<code>make_last_layers</code>函数，这就是5次卷积和最后结果卷积。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">make_last_layers</span>(<span class=\"params\">filters_list, in_filters, out_filter</span>):</span></span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">    5次卷积+2次卷积</span></span><br><span class=\"line\"><span class=\"string\">    :param filters_list:中间过渡通道数</span></span><br><span class=\"line\"><span class=\"string\">    :param in_filters: 输入通道数</span></span><br><span class=\"line\"><span class=\"string\">    :param out_filter: 输出通道数</span></span><br><span class=\"line\"><span class=\"string\">    :return: 5次卷积+2次卷积的模型</span></span><br><span class=\"line\"><span class=\"string\">    &quot;&quot;&quot;</span></span><br><span class=\"line\">    m = nn.ModuleList([</span><br><span class=\"line\">        <span class=\"comment\"># 1*1卷积调整通道（降低通道数）</span></span><br><span class=\"line\">        conv2d(in_filters, filters_list[<span class=\"number\">0</span>], <span class=\"number\">1</span>),</span><br><span class=\"line\">        <span class=\"comment\"># 3*3卷积提取特征</span></span><br><span class=\"line\">        conv2d(filters_list[<span class=\"number\">0</span>], filters_list[<span class=\"number\">1</span>], <span class=\"number\">3</span>),</span><br><span class=\"line\">        <span class=\"comment\"># 1*1卷积调整通道（降低通道数）</span></span><br><span class=\"line\">        conv2d(filters_list[<span class=\"number\">1</span>], filters_list[<span class=\"number\">0</span>], <span class=\"number\">1</span>),</span><br><span class=\"line\">        <span class=\"comment\"># 3*3卷积提取特征</span></span><br><span class=\"line\">        conv2d(filters_list[<span class=\"number\">0</span>], filters_list[<span class=\"number\">1</span>], <span class=\"number\">3</span>),</span><br><span class=\"line\">        <span class=\"comment\"># 1*1卷积调整通道</span></span><br><span class=\"line\">        conv2d(filters_list[<span class=\"number\">1</span>], filters_list[<span class=\"number\">0</span>], <span class=\"number\">1</span>),</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># 接下来两此卷积用来分类预测和回归预测</span></span><br><span class=\"line\">        conv2d(filters_list[<span class=\"number\">0</span>], filters_list[<span class=\"number\">1</span>], <span class=\"number\">3</span>),</span><br><span class=\"line\">        nn.Conv2d(filters_list[<span class=\"number\">1</span>], out_filter, kernel_size=<span class=\"number\">1</span>,</span><br><span class=\"line\">                                        stride=<span class=\"number\">1</span>, padding=<span class=\"number\">0</span>, bias=<span class=\"literal\">True</span>)</span><br><span class=\"line\">    ])</span><br><span class=\"line\">    <span class=\"keyword\">return</span> m</span><br></pre></td></tr></table></figure>\n<p>$1*1$卷积是很有效减少通道数，从而减少参数，这对电脑减轻了不小负担。</p>\n<p>接下来是$13*13$特征层的提取：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># embedding1</span></span><br><span class=\"line\"><span class=\"comment\"># 此值为75</span></span><br><span class=\"line\">final_out_filter1 = <span class=\"built_in\">len</span>(config[<span class=\"string\">&quot;yolo&quot;</span>][<span class=\"string\">&quot;anchors&quot;</span>][<span class=\"number\">1</span>]) * (<span class=\"number\">5</span> + config[<span class=\"string\">&quot;yolo&quot;</span>][<span class=\"string\">&quot;classes&quot;</span>])</span><br><span class=\"line\"><span class=\"comment\"># 1*1卷积调整通道数（从512调节成256通道）</span></span><br><span class=\"line\">self.last_layer1_conv = conv2d(<span class=\"number\">512</span>, <span class=\"number\">256</span>, <span class=\"number\">1</span>)</span><br><span class=\"line\"><span class=\"comment\"># 上采样</span></span><br><span class=\"line\">self.last_layer1_upsample = nn.Upsample(scale_factor=<span class=\"number\">2</span>, mode=<span class=\"string\">&#x27;nearest&#x27;</span>)</span><br><span class=\"line\"><span class=\"comment\"># 5次卷积+2次卷积</span></span><br><span class=\"line\">self.last_layer1 = make_last_layers([<span class=\"number\">256</span>, <span class=\"number\">512</span>], out_filters[-<span class=\"number\">2</span>] + <span class=\"number\">256</span>, final_out_filter1)</span><br></pre></td></tr></table></figure>\n<p>在使用上采样之前需要用$1*1$卷积核来调整通道，这样可以保证接下来上采样时通道一致。</p>\n<p>由于使用上采样，所以我们需要上采样函数，幸运的是pytorch提供了上采样，所以我们免去了这个过程。</p>\n<p>最后就是剩下特征层，也是同样道理：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># embedding2</span></span><br><span class=\"line\"><span class=\"comment\"># 此值为75</span></span><br><span class=\"line\">final_out_filter2 = <span class=\"built_in\">len</span>(config[<span class=\"string\">&quot;yolo&quot;</span>][<span class=\"string\">&quot;anchors&quot;</span>][<span class=\"number\">2</span>]) * (<span class=\"number\">5</span> + config[<span class=\"string\">&quot;yolo&quot;</span>][<span class=\"string\">&quot;classes&quot;</span>])</span><br><span class=\"line\"><span class=\"comment\"># 1*1卷积调整通道数（从256调节成128通道）</span></span><br><span class=\"line\">self.last_layer2_conv = conv2d(<span class=\"number\">256</span>, <span class=\"number\">128</span>, <span class=\"number\">1</span>)</span><br><span class=\"line\"><span class=\"comment\"># 上采样</span></span><br><span class=\"line\">self.last_layer2_upsample = nn.Upsample(scale_factor=<span class=\"number\">2</span>, mode=<span class=\"string\">&#x27;nearest&#x27;</span>)</span><br><span class=\"line\"><span class=\"comment\"># 5次卷积+2次卷积</span></span><br><span class=\"line\">self.last_layer2 = make_last_layers([<span class=\"number\">128</span>, <span class=\"number\">256</span>], out_filters[-<span class=\"number\">3</span>] + <span class=\"number\">128</span>, final_out_filter2)</span><br></pre></td></tr></table></figure>\n<h4 id=\"前向传播-1\"><a href=\"#前向传播-1\" class=\"headerlink\" title=\"前向传播\"></a>前向传播</h4><p>看到这里，所有初始化都完成了，也就是网络结构部分完成了，剩下就是其他的代码，就比如前向传播的整体还没写。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">forward</span>(<span class=\"params\">self, x</span>):</span></span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">    前向传播</span></span><br><span class=\"line\"><span class=\"string\">    :param x: 输入</span></span><br><span class=\"line\"><span class=\"string\">    :return:</span></span><br><span class=\"line\"><span class=\"string\">    &quot;&quot;&quot;</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># 函数中定义函数</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">_branch</span>(<span class=\"params\">last_layer, layer_in</span>):</span></span><br><span class=\"line\">        <span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">        分开卷积过程</span></span><br><span class=\"line\"><span class=\"string\">        :param last_layer: 上一层结果</span></span><br><span class=\"line\"><span class=\"string\">        :param layer_in:会存入卷积过程的结果并会返回出去</span></span><br><span class=\"line\"><span class=\"string\">        :return: out_branch是最后结果</span></span><br><span class=\"line\"><span class=\"string\">        &quot;&quot;&quot;</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> i, e <span class=\"keyword\">in</span> <span class=\"built_in\">enumerate</span>(last_layer):</span><br><span class=\"line\">            layer_in = e(layer_in)</span><br><span class=\"line\">            <span class=\"keyword\">if</span> i == <span class=\"number\">4</span>:</span><br><span class=\"line\">                <span class=\"comment\"># 第五次卷积的时候取出来</span></span><br><span class=\"line\">                out_branch = layer_in</span><br><span class=\"line\">        <span class=\"keyword\">return</span> layer_in, out_branch</span><br></pre></td></tr></table></figure>\n<p>没错，直接套娃就可以，我挺喜欢这样写，因为这样写可以让整个函数在另一个函数周期里，外函数“死亡”时，里面的函数也会“死亡”。之所以定义这个函数是把第五次卷积结果拿出来，因为需要上采样传递给其他层。</p>\n<p>继续写，在DarkNet中我们可以获得到三种特征图。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 获取三种特征图</span></span><br><span class=\"line\">x2, x1, x0 = self.backbone(x)</span><br></pre></td></tr></table></figure>\n<p>现在可以正常正向传播了，首先第一套特征图：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 5次卷积+2次卷积</span></span><br><span class=\"line\"><span class=\"comment\"># 直接可以得到卷积神经网络结果</span></span><br><span class=\"line\">out0, out0_branch = _branch(self.last_layer0, x0)</span><br></pre></td></tr></table></figure>\n<p>第二套特征图使用计算之前，需要调整通道数、上采样、堆叠：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 1*1卷积调整通道数（从512调节成256通道）</span></span><br><span class=\"line\">x1_in = self.last_layer1_conv(out0_branch)</span><br><span class=\"line\"><span class=\"comment\"># 上采样</span></span><br><span class=\"line\">x1_in = self.last_layer1_upsample(x1_in)</span><br><span class=\"line\"><span class=\"comment\"># 堆叠</span></span><br><span class=\"line\">x1_in = torch.cat([x1_in, x1], <span class=\"number\">1</span>)</span><br><span class=\"line\"><span class=\"comment\"># 5次卷积+2次卷积</span></span><br><span class=\"line\"><span class=\"comment\"># 直接可以得到卷积神经网络结果</span></span><br><span class=\"line\">out1, out1_branch = _branch(self.last_layer1, x1_in)</span><br></pre></td></tr></table></figure>\n<p>第三套同样如此：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 1*1卷积调整通道数（从256调节成128通道）</span></span><br><span class=\"line\">x2_in = self.last_layer2_conv(out1_branch)</span><br><span class=\"line\"><span class=\"comment\"># 上采样</span></span><br><span class=\"line\">x2_in = self.last_layer2_upsample(x2_in)</span><br><span class=\"line\"><span class=\"comment\"># 堆叠</span></span><br><span class=\"line\">x2_in = torch.cat([x2_in, x2], <span class=\"number\">1</span>)</span><br><span class=\"line\"><span class=\"comment\"># 5次卷积+2次卷积</span></span><br><span class=\"line\"><span class=\"comment\"># 直接可以得到卷积神经网络结果</span></span><br><span class=\"line\">out2, _ = _branch(self.last_layer2, x2_in)        </span><br></pre></td></tr></table></figure>\n<p>最后直接输出：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">return</span> out0, out1, out2</span><br></pre></td></tr></table></figure>\n<p>以上我们的网络就搭建完毕了，但还缺少解码部分以及损失函数，所以继续写吧。</p>\n<h3 id=\"解码\"><a href=\"#解码\" class=\"headerlink\" title=\"解码\"></a>解码</h3><p>将解码也定义一个层，这样就方便后续的处理：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">DecodeBox</span>(<span class=\"params\">nn.Module</span>):</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span>(<span class=\"params\">self, anchors, num_classes, img_size</span>):</span></span><br><span class=\"line\">        <span class=\"built_in\">super</span>(DecodeBox, self).__init__()</span><br><span class=\"line\">        self.anchors = anchors</span><br><span class=\"line\">        self.num_anchors = <span class=\"built_in\">len</span>(anchors)</span><br><span class=\"line\">        self.num_classes = num_classes</span><br><span class=\"line\">        self.bbox_attrs = <span class=\"number\">5</span> + num_classes</span><br><span class=\"line\">        <span class=\"comment\"># 保存图像大小</span></span><br><span class=\"line\">        self.img_size = img_size</span><br></pre></td></tr></table></figure>\n<p>前向传播部分：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">forward</span>(<span class=\"params\">self, <span class=\"built_in\">input</span></span>):</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># 此时的形状是（N, 3*(20+5), size, size）</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># 获取图片数量（N,）</span></span><br><span class=\"line\">    batch_size = <span class=\"built_in\">input</span>.size(<span class=\"number\">0</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># 获取图片几行网格</span></span><br><span class=\"line\">    input_height = <span class=\"built_in\">input</span>.size(<span class=\"number\">2</span>)</span><br><span class=\"line\">    <span class=\"comment\"># 获取图片几列网格</span></span><br><span class=\"line\">    input_width = <span class=\"built_in\">input</span>.size(<span class=\"number\">3</span>)</span><br></pre></td></tr></table></figure>\n<p>当然还没完，我们还要获得图片步长，也就是图片每个网格内有多少像素点（感受野）：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 计算步长</span></span><br><span class=\"line\"><span class=\"comment\"># 每个网格内有多少像素点</span></span><br><span class=\"line\"><span class=\"comment\"># 图片尺寸 / 每个网格大小</span></span><br><span class=\"line\">stride_h = self.img_size[<span class=\"number\">1</span>] / input_height</span><br><span class=\"line\">stride_w = self.img_size[<span class=\"number\">0</span>] / input_width</span><br></pre></td></tr></table></figure>\n<p>我们之前定义了三种先验框，所以注意最开始的先验框的单位是像素，我们需要将这个改变成根据步长的百分比，也就是归一到特征层上：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 归一到特征层上</span></span><br><span class=\"line\">scaled_anchors = [(anchor_width / stride_w, anchor_height / stride_h) <span class=\"keyword\">for</span> anchor_width, anchor_height <span class=\"keyword\">in</span> self.anchors]</span><br></pre></td></tr></table></figure>\n<p>因为有三套先验框，所以这个循环会执行三次。</p>\n<p>还需要通道转换，<code>.permute(0, 1, 3, 4, 2).contiguous()</code>：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># （N, 3*(20+5), size, size）-&gt;（N, 3 , size, size, 20+5）</span></span><br><span class=\"line\">prediction = <span class=\"built_in\">input</span>.view(batch_size, self.num_anchors,</span><br><span class=\"line\">                        self.bbox_attrs, input_height, input_width).permute(<span class=\"number\">0</span>, <span class=\"number\">1</span>, <span class=\"number\">3</span>, <span class=\"number\">4</span>, <span class=\"number\">2</span>).contiguous()</span><br></pre></td></tr></table></figure>\n<p>处理一下先验框的中心参数，怕是遇到负数，所以使用<code>sigmoid()</code>函数过滤一下，这样百分百是正数且不会大于1：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 先验框的中心位置的调整参数</span></span><br><span class=\"line\">x = torch.sigmoid(prediction[..., <span class=\"number\">0</span>])</span><br><span class=\"line\">y = torch.sigmoid(prediction[..., <span class=\"number\">1</span>])</span><br></pre></td></tr></table></figure>\n<p>还有宽高参数：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 先验框的宽高调整参数</span></span><br><span class=\"line\">w = prediction[..., <span class=\"number\">2</span>]  <span class=\"comment\"># Width</span></span><br><span class=\"line\">h = prediction[..., <span class=\"number\">3</span>]  <span class=\"comment\"># Height</span></span><br></pre></td></tr></table></figure>\n<p>以及物体置信度和种类置信度，听说在之前yolo使用的是<code>softmax()</code>函数，这样就会产生独立事件，有时会出bug，<code>sigmoid()</code>可以解决这个问题：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 获得置信度，是否有物体</span></span><br><span class=\"line\">conf = torch.sigmoid(prediction[..., <span class=\"number\">4</span>])</span><br><span class=\"line\"><span class=\"comment\"># 获得种类置信度</span></span><br><span class=\"line\">pred_cls = torch.sigmoid(prediction[..., <span class=\"number\">5</span>:])</span><br></pre></td></tr></table></figure>\n<p>先验框问题解决了，接下来就生成出来，根据网格左上角生成中心：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 生成网格，先验框中心，网格左上角</span></span><br><span class=\"line\">grid_x = torch.linspace(<span class=\"number\">0</span>, input_width - <span class=\"number\">1</span>, input_width).repeat(input_width, <span class=\"number\">1</span>).repeat(batch_size * self.num_anchors, <span class=\"number\">1</span>, <span class=\"number\">1</span>).view(x.shape).<span class=\"built_in\">type</span>(FloatTensor)</span><br><span class=\"line\">grid_y = torch.linspace(<span class=\"number\">0</span>, input_height - <span class=\"number\">1</span>, input_height).repeat(input_height, <span class=\"number\">1</span>).t().repeat(batch_size * self.num_anchors, <span class=\"number\">1</span>, <span class=\"number\">1</span>).view(y.shape).<span class=\"built_in\">type</span>(FloatTensor)</span><br></pre></td></tr></table></figure>\n<p>也就是从网格的左上角，每隔size单位生成一个，形状是<code>（N, 3, size, size）</code></p>\n<p>再生成先验框的宽和高，根据自己设定的原始先验框：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 生成先验框的宽高</span></span><br><span class=\"line\">anchor_w = FloatTensor(scaled_anchors).index_select(<span class=\"number\">1</span>, LongTensor([<span class=\"number\">0</span>]))</span><br><span class=\"line\">anchor_h = FloatTensor(scaled_anchors).index_select(<span class=\"number\">1</span>, LongTensor([<span class=\"number\">1</span>]))</span><br><span class=\"line\">anchor_w = anchor_w.repeat(batch_size, <span class=\"number\">1</span>).repeat(<span class=\"number\">1</span>, <span class=\"number\">1</span>, input_height * input_width).view(w.shape)</span><br><span class=\"line\">anchor_h = anchor_h.repeat(batch_size, <span class=\"number\">1</span>).repeat(<span class=\"number\">1</span>, <span class=\"number\">1</span>, input_height * input_width).view(h.shape)</span><br></pre></td></tr></table></figure>\n<p>这是最后的调整了：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 计算调整后的先验框中心与宽高</span></span><br><span class=\"line\">pred_boxes = FloatTensor(prediction[..., :<span class=\"number\">4</span>].shape)</span><br><span class=\"line\">pred_boxes[..., <span class=\"number\">0</span>] = x.data + grid_x</span><br><span class=\"line\">pred_boxes[..., <span class=\"number\">1</span>] = y.data + grid_y</span><br><span class=\"line\">pred_boxes[..., <span class=\"number\">2</span>] = torch.exp(w.data) * anchor_w</span><br><span class=\"line\">pred_boxes[..., <span class=\"number\">3</span>] = torch.exp(h.data) * anchor_h</span><br></pre></td></tr></table></figure>\n<p>之前我们获得的先验框中心是一个0到1之间的数字，加上网格的次序就可以直接检查出是哪个网格中的中心。</p>\n<p>结尾：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 用于将输出调整为相对于416x416的大小</span></span><br><span class=\"line\">_scale = torch.Tensor([stride_w, stride_h] * <span class=\"number\">2</span>).<span class=\"built_in\">type</span>(FloatTensor)</span><br><span class=\"line\">output = torch.cat((pred_boxes.view(batch_size, -<span class=\"number\">1</span>, <span class=\"number\">4</span>) * _scale,conf.view(batch_size, -<span class=\"number\">1</span>, <span class=\"number\">1</span>), pred_cls.view(batch_size, -<span class=\"number\">1</span>, self.num_classes)), -<span class=\"number\">1</span>)</span><br><span class=\"line\"><span class=\"keyword\">return</span> output.data</span><br></pre></td></tr></table></figure>\n<blockquote>\n<p>参考：</p>\n<p><a href=\"https://blog.csdn.net/weixin_44791964/article/details/105310627\">https://blog.csdn.net/weixin_44791964/article/details/105310627</a></p>\n<p><a href=\"https://www.bilibili.com/video/BV1Hp4y1y788\">https://www.bilibili.com/video/BV1Hp4y1y788</a></p>\n</blockquote>\n","more":"<p>YOLO是“You Only Look Once”的缩写，YOLO将物体检测作为回归问题求解，是一个对象检测算法的名字，这是Redmon等人在2016年的一篇研究论文中命名的。</p>\n<p><img src=\"/2021/02/%E7%94%A8Pytorch%E6%90%AD%E5%BB%BAyolov3/20210207195622.jpg\" alt=\"20191020111518954\" style=\"zoom:50%;\"></p>\n<p>本篇文章会一步一步搭建起yolov3的神经网络，详细内容会具体分析。</p>\n<h2 id=\"YOLO介绍\"><a href=\"#YOLO介绍\" class=\"headerlink\" title=\"YOLO介绍\"></a>YOLO介绍</h2><p>目标检测（object detection）是一个因近年来深度学习的发展而受益颇多的领域，近年来，人们开发了多种目标检测算法，其中包括YOLO、SSD、Mask-RCNN和RetinaNet。此篇文章使用PyTorch并基于YOLO v3来实现一个目标检测器，这是一种速度更快的目标检测算法。</p>\n<p>YOLO是“You Only Look Once”的缩写，YOLO将物体检测作为回归问题求解，是一个对象检测算法的名字，这是Redmon等人在2016年的一篇研究论文中命名的：</p>\n<blockquote>\n<p><strong><em>Redmon J , Divvala S , Girshick R , et al. You Only Look Once: Unified, Real-Time Object Detection[J]. 2015.</em></strong></p>\n</blockquote>\n<h2 id=\"YOLO结构\"><a href=\"#YOLO结构\" class=\"headerlink\" title=\"YOLO结构\"></a>YOLO结构</h2><p>以下为yolo的整体结构：</p>\n<p><img src=\"/2021/02/%E7%94%A8Pytorch%E6%90%AD%E5%BB%BAyolov3/20191020111518954.jpg\" alt=\"20191020111518954\"></p>\n<p>这张图分成两部分，左边虚线框起来的是<strong>主干特征提取</strong>部分。</p>\n<h3 id=\"主干特征提取\"><a href=\"#主干特征提取\" class=\"headerlink\" title=\"主干特征提取\"></a>主干特征提取</h3><p>这部分称作Darknet-53，主干特征提取顾名思义，用来提取图片的特征。</p>\n<p>输入是需要一张$416 <em> 416 </em> 3$大小的图片，然后是不断卷积的过程，如果仔细看会发现图片的高和宽不断被压缩，通道数却不断扩张，这是一个<strong>下采样</strong>过程。</p>\n<p>经过下采样之后，会获得<strong>特征层</strong>（用来表述图片的特征），下图是我们需要的特征层。</p>\n<p><img src=\"/2021/02/%E7%94%A8Pytorch%E6%90%AD%E5%BB%BAyolov3/2019.jpg\" alt=\"2019\"></p>\n<p>由此可见保留下来的特征层为：$52<em>52</em>256$、$26<em>26</em>512$、$13<em>13</em>1024$三种尺寸，这三种特征层是即将传入右边的部分。</p>\n<p><img src=\"/2021/02/%E7%94%A8Pytorch%E6%90%AD%E5%BB%BAyolov3/1437686-20200102124439991-778619867.png\" alt=\"1437686-20200102124439991-778619867\"></p>\n<p>每个特征图可以看作一个“条目”，因为一个“条目”有一些信息，如下图所示：</p>\n<p><img src=\"/2021/02/%E7%94%A8Pytorch%E6%90%AD%E5%BB%BAyolov3/1437686-20191230192708001-430020660.png\" alt=\"1437686-20191230192708001-430020660\"></p>\n<h3 id=\"其他部分\"><a href=\"#其他部分\" class=\"headerlink\" title=\"其他部分\"></a>其他部分</h3><p>这里的其他部分泛指右边部分的网络。</p>\n<p>经过主干特征提取后，我们获得了三种尺寸的特征层，分别对三种尺寸的特征层处理。</p>\n<h4 id=\"处理-13131024-特征层\"><a href=\"#处理-13131024-特征层\" class=\"headerlink\" title=\"处理$13131024$特征层\"></a>处理$13<em>13</em>1024$特征层</h4><p>首先说一下$13<em>13</em>1024$特征层的处理：</p>\n<p><img src=\"/2021/02/%E7%94%A8Pytorch%E6%90%AD%E5%BB%BAyolov3/10201.jpg\" alt=\"10201\"></p>\n<p>可以看到$13<em>13</em>1024$特征层经过了5次卷积后传到了两个方向。</p>\n<p>右边的粉色部分是<strong>分类预测</strong>和<strong>回归预测</strong>，其实就是两次卷积，最后会获得$13<em>13</em>75$大小的，再经过一个分解变成$13<em>13</em>3<em>25$，即$13</em>13<em>3</em>(20 + 1 + 4)$，这个过程其实就是化成$13<em>13$的网格，每个网格有<strong>3个</strong>预测出的<em>*先验框</em></em>，接下来会根据判断属于那种框的尺寸。</p>\n<p>之所以会把25分成三部分，其实就gailv是20个物体分类，由于使用<strong>voc数据集</strong>，所以会有20个，<strong>coco训练集</strong>会出现80个，简单来讲20个<strong>置信度（属于哪个类的概率）</strong>会分类属于哪个类；1是指是否有物体；4是对框的调整。</p>\n<p>注意$13<em>13</em>1024$特征层还有一个传递方向，这时需要一个<strong>上采样</strong>，也就是扩增长宽，减小通道数。经过上采样之后会和$26<em>26</em>512$特征层进行<strong>堆叠（增加通道数）</strong>。</p>\n<h4 id=\"处理-2626512-特征层\"><a href=\"#处理-2626512-特征层\" class=\"headerlink\" title=\"处理$2626512$特征层\"></a>处理$26<em>26</em>512$特征层</h4><p>$26<em>26</em>512$特征层在和经过上采样的$13<em>13</em>1024$特征层进行堆叠后，会形成一个新的“特征金字塔”，当然这个特征金字塔还会继续堆叠。</p>\n<p><img src=\"/2021/02/%E7%94%A8Pytorch%E6%90%AD%E5%BB%BAyolov3/0111.jpg\" alt=\"0111\"></p>\n<p>后面的部分就一样了，再5次卷积，在最后进行分类预测和回归预测，会出现相同操作。</p>\n<p>与此同时会把5次卷积后继续向上传递，并有个上采样。</p>\n<p>$52<em>52</em>256$特征层同样如此就不再赘述。</p>\n<p>输出结果：</p>\n<p>$t_x$和$t_y$是被检测物体的中心位置，$t_w$和$t_h$是方框的尺寸，$p_0$是<strong>物体置信度</strong>，代表了这个区域内有物体的概率，$p_1-&gt;p_c$是分类置信度，哪类概率高，就属于什么物体。最后还有个B，这个是锚框个数，说明了这个区域可以最多检测出B个物体。</p>\n<h3 id=\"Anchor-Boxes\"><a href=\"#Anchor-Boxes\" class=\"headerlink\" title=\"Anchor Boxes\"></a>Anchor Boxes</h3><p>有的翻译成锚框，有的翻译成先验框，这里我就叫它锚框了。</p>\n<p>YOLO不能直接预测边界框的宽度和高度，这会导致训练过程中出现不稳定的变化。大多数现代目标检测器会预测对数空间转换，或者只是偏移到称为锚点的预定义默认锚框。YOLO-v3具有三个锚点，可在每个细胞单元格上预测三个边界框。</p>\n<p>那么真正的方框是怎么预测出的？先看下面的公式：</p>\n<script type=\"math/tex; mode=display\">\nb_x = \\sigma(t_x) + c_x\\\\\nb_y = \\sigma(t_y) + c_y\\\\\nb_w = p_w e^{t_w}\\\\\nb_h = p_h e^{t_h}</script><p>前两个公式是预测中心点，经过$\\sigma()$函数后就会稳定在0和1之间。$c_x$和$c_y$是第几个方框，例如上图中红色框的这两值都是6。</p>\n<p>后面两个公式预测宽度，$p_w$和$p_h$是锚框的尺寸，直接乘就可以。</p>\n<p>还没有完，这只完成了一半，接着看：</p>\n<p><img src=\"/2021/02/%E7%94%A8Pytorch%E6%90%AD%E5%BB%BAyolov3/1437686-20200102121153977-382469172.png\" alt=\"1437686-20200102121153977-382469172\"></p>\n<p>中心点需要还乘上对应的网格宽度，方框需要带入e指数中。</p>\n<p>补一张更漂亮的图。</p>\n<p><img src=\"/2021/02/%E7%94%A8Pytorch%E6%90%AD%E5%BB%BAyolov3/20190509224534499.png\" alt=\"v2-fb8b964727ccfea93345ba1361c4c8a3_720w\"></p>\n<h2 id=\"YOLO预测原理\"><a href=\"#YOLO预测原理\" class=\"headerlink\" title=\"YOLO预测原理\"></a>YOLO预测原理</h2><p>YOLO将图片分成了3种检测，检测的区别是按照分割区域的大小。</p>\n<p>首先输入图片的大小要定下为$416*416$大小，如果不合适就要补上缺失部分，目的就是防止失真。</p>\n<p>将图片分别分成$52<em>52$、$26</em>26$、$13*13$、三种尺寸的网格，针对的识别三种尺寸，也就对应上了上部分的三种输出结果。</p>\n<p>每个输出结果的维度为<code>(N, 75 * 3, x, x)</code>，x为尺寸分别对应52、26、13三种，N代表样本数，75 * 3这个数在上面提过了就不再多说。</p>\n<p>最后的最终的预测结构后还要进行得分排序与非极大抑制筛选。</p>\n<p>用$7<em>7$来举例（我只找到$7</em>7$的例子，我又懒得去做$13<em>13$的图），下面将一幅图分成$7</em>7$网格，共49部分。</p>\n<p><img src=\"/2021/02/%E7%94%A8Pytorch%E6%90%AD%E5%BB%BAyolov3/8694a4c27d1ed21bc6ed15f6c38952c250da3fee.jpeg\" alt=\"8694a4c27d1ed21bc6ed15f6c38952c250da3fee\"></p>\n<p>对于每个网格点，都会预测一个边界框和与每个类别（汽车，行人，交通信号灯等）相对应的概率，每个网络点负责一个区域的检测。</p>\n<h2 id=\"程序\"><a href=\"#程序\" class=\"headerlink\" title=\"程序\"></a>程序</h2><p>首先搭建DarkNet大概框架：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">DarkNet</span>(<span class=\"params\">nn.Module</span>):</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span>(<span class=\"params\">self, layers</span>):</span></span><br><span class=\"line\">        <span class=\"built_in\">super</span>(DarkNet, self).__init__()</span><br></pre></td></tr></table></figure>\n<p>DarkNet继承了pytorch中的模型，目的使用一些相同的框架函数。</p>\n<h3 id=\"初始的卷积层\"><a href=\"#初始的卷积层\" class=\"headerlink\" title=\"初始的卷积层\"></a>初始的卷积层</h3><p>继续根据整体结构来加入每一层，首先卷积层：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 设置卷积核个数，也是卷积后的通道数</span></span><br><span class=\"line\">self.inplanes = <span class=\"number\">32</span></span><br><span class=\"line\"><span class=\"comment\"># 大概配置参数是：</span></span><br><span class=\"line\"><span class=\"comment\"># 输入通道数、输出通道数（卷积核个数）、卷积核尺寸、步长填充、是否加入偏置</span></span><br><span class=\"line\"><span class=\"comment\"># 此时下面的配置是same卷积，没有破坏图像原本的大小</span></span><br><span class=\"line\">self.conv1 = nn.Conv2d(<span class=\"number\">3</span>, self.inplanes, kernel_size=<span class=\"number\">3</span>, stride=<span class=\"number\">1</span>, padding=<span class=\"number\">1</span>, bias=<span class=\"literal\">False</span>)</span><br></pre></td></tr></table></figure>\n<p>标准化和激活函数：</p>\n<p><strong>注意</strong>：这里使用的是LeakyReLU，这样好处是，负数区域还存在值，同时也有斜率。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># BatchNorm标准化，加速收敛速度及稳定性的算法</span></span><br><span class=\"line\">self.bn1 = nn.BatchNorm2d(self.inplanes)</span><br><span class=\"line\"><span class=\"comment\"># 设置激活函数</span></span><br><span class=\"line\">self.relu1 = nn.LeakyReLU(<span class=\"number\">0.1</span>)</span><br></pre></td></tr></table></figure>\n<p>以上操作建立了一个卷积层，最后生成的维度为：<code>(N, 416, 416, 32)</code></p>\n<h3 id=\"残差块\"><a href=\"#残差块\" class=\"headerlink\" title=\"残差块\"></a>残差块</h3><p>先制做出残差块的函数，返回值是一个残差块：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">_make_layer</span>(<span class=\"params\">self, planes, blocks</span>):</span></span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">    制作一个结构块</span></span><br><span class=\"line\"><span class=\"string\">    :param planes: 是个列表，第一位置是输入通道数，第二位置是输出通道数</span></span><br><span class=\"line\"><span class=\"string\">    :param blocks: 残差块堆叠个数</span></span><br><span class=\"line\"><span class=\"string\">    :return:</span></span><br><span class=\"line\"><span class=\"string\">    &quot;&quot;&quot;</span></span><br><span class=\"line\">    layers = []</span><br><span class=\"line\">    <span class=\"comment\"># 下采样，步长为2，卷积核大小为3，填充为1</span></span><br><span class=\"line\">    <span class=\"comment\"># 这样长宽就可以压缩</span></span><br><span class=\"line\">    layers.append((<span class=\"string\">&quot;ds_conv&quot;</span>, nn.Conv2d(self.inplanes, planes[<span class=\"number\">1</span>], kernel_size=<span class=\"number\">3</span>,</span><br><span class=\"line\">                            stride=<span class=\"number\">2</span>, padding=<span class=\"number\">1</span>, bias=<span class=\"literal\">False</span>)))</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># BatchNorm标准化</span></span><br><span class=\"line\">    layers.append((<span class=\"string\">&quot;ds_bn&quot;</span>, nn.BatchNorm2d(planes[<span class=\"number\">1</span>])))</span><br><span class=\"line\">    <span class=\"comment\"># 设置激活函数</span></span><br><span class=\"line\">    layers.append((<span class=\"string\">&quot;ds_relu&quot;</span>, nn.LeakyReLU(<span class=\"number\">0.1</span>)))</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># 加入darknet模块</span></span><br><span class=\"line\">    self.inplanes = planes[<span class=\"number\">1</span>]</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># 堆叠残差块</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"number\">0</span>, blocks):</span><br><span class=\"line\">        <span class=\"comment\"># 添加一个网络</span></span><br><span class=\"line\">        <span class=\"comment\"># layers是个列表，里面元素是元组，元组元素是字符串和BasicBlock网络块</span></span><br><span class=\"line\">        layers.append((<span class=\"string\">&quot;residual_&#123;&#125;&quot;</span>.<span class=\"built_in\">format</span>(i), BasicBlock(self.inplanes, planes)))</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># 打包网络</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> nn.Sequential(OrderedDict(layers))</span><br></pre></td></tr></table></figure>\n<p>OrderedDict是有序字典，虽然平常不常用，但在这里使用是最合适的，记得导入包：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> collections <span class=\"keyword\">import</span> OrderedDict</span><br></pre></td></tr></table></figure>\n<p>最核心的部分是BasicBlock类，这里是残差网络相加的地方：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 基本的darknet块（残差块的核心）</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">BasicBlock</span>(<span class=\"params\">nn.Module</span>):</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span>(<span class=\"params\">self, inplanes, planes</span>):</span></span><br><span class=\"line\">        <span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">        残差块</span></span><br><span class=\"line\"><span class=\"string\">        :param inplanes:</span></span><br><span class=\"line\"><span class=\"string\">        :param planes: 下采样部分</span></span><br><span class=\"line\"><span class=\"string\">        &quot;&quot;&quot;</span></span><br><span class=\"line\">        <span class=\"built_in\">super</span>(BasicBlock, self).__init__()</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># 定义了两组卷积操作</span></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># 最开始是卷积核形状为1的卷积</span></span><br><span class=\"line\">        <span class=\"comment\"># 目的是下降通道数</span></span><br><span class=\"line\">        self.conv1 = nn.Conv2d(inplanes, planes[<span class=\"number\">0</span>], kernel_size=<span class=\"number\">1</span>,</span><br><span class=\"line\">                               stride=<span class=\"number\">1</span>, padding=<span class=\"number\">0</span>, bias=<span class=\"literal\">False</span>)</span><br><span class=\"line\">        <span class=\"comment\"># BatchNorm标准化</span></span><br><span class=\"line\">        self.bn1 = nn.BatchNorm2d(planes[<span class=\"number\">0</span>])</span><br><span class=\"line\">        <span class=\"comment\"># 设置激活函数</span></span><br><span class=\"line\">        self.relu1 = nn.LeakyReLU(<span class=\"number\">0.1</span>)</span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"comment\"># 为了保证残差成功，必须使用same卷积</span></span><br><span class=\"line\">        <span class=\"comment\"># 这里还将通道数提升了</span></span><br><span class=\"line\">        self.conv2 = nn.Conv2d(planes[<span class=\"number\">0</span>], planes[<span class=\"number\">1</span>], kernel_size=<span class=\"number\">3</span>,</span><br><span class=\"line\">                               stride=<span class=\"number\">1</span>, padding=<span class=\"number\">1</span>, bias=<span class=\"literal\">False</span>)</span><br><span class=\"line\">        <span class=\"comment\"># BatchNorm标准化</span></span><br><span class=\"line\">        self.bn2 = nn.BatchNorm2d(planes[<span class=\"number\">1</span>])</span><br><span class=\"line\">        <span class=\"comment\"># 设置激活函数</span></span><br><span class=\"line\">        self.relu2 = nn.LeakyReLU(<span class=\"number\">0.1</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">forward</span>(<span class=\"params\">self, x</span>):</span></span><br><span class=\"line\">        <span class=\"comment\"># 保存残差”边“</span></span><br><span class=\"line\">        residual = x</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># 传统艺能</span></span><br><span class=\"line\">        </span><br><span class=\"line\">        out = self.conv1(x)</span><br><span class=\"line\">        out = self.bn1(out)</span><br><span class=\"line\">        out = self.relu1(out)</span><br><span class=\"line\"></span><br><span class=\"line\">        out = self.conv2(out)</span><br><span class=\"line\">        out = self.bn2(out)</span><br><span class=\"line\">        out = self.relu2(out)</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># 网络最后加上残差边</span></span><br><span class=\"line\">        out += residual</span><br><span class=\"line\">        <span class=\"keyword\">return</span> out</span><br></pre></td></tr></table></figure>\n<p>由于里面先进行1卷积，再3卷积，这样可以减少参数量，</p>\n<p>初始化剩下的东西：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">   <span class=\"comment\"># </span></span><br><span class=\"line\">self.layers_out_filters = [<span class=\"number\">64</span>, <span class=\"number\">128</span>, <span class=\"number\">256</span>, <span class=\"number\">512</span>, <span class=\"number\">1024</span>]</span><br><span class=\"line\"></span><br><span class=\"line\">   <span class=\"comment\"># 进行权值初始化</span></span><br><span class=\"line\">   <span class=\"keyword\">for</span> m <span class=\"keyword\">in</span> self.modules():</span><br><span class=\"line\">       <span class=\"keyword\">if</span> <span class=\"built_in\">isinstance</span>(m, nn.Conv2d):</span><br><span class=\"line\">           n = m.kernel_size[<span class=\"number\">0</span>] * m.kernel_size[<span class=\"number\">1</span>] * m.out_channels</span><br><span class=\"line\">           m.weight.data.normal_(<span class=\"number\">0</span>, math.sqrt(<span class=\"number\">2.</span> / n))</span><br><span class=\"line\">       <span class=\"keyword\">elif</span> <span class=\"built_in\">isinstance</span>(m, nn.BatchNorm2d):</span><br><span class=\"line\">           m.weight.data.fill_(<span class=\"number\">1</span>)</span><br><span class=\"line\">           m.bias.data.zero_()</span><br></pre></td></tr></table></figure>\n<h3 id=\"前向传播\"><a href=\"#前向传播\" class=\"headerlink\" title=\"前向传播\"></a>前向传播</h3><p>前向传播的时候我们需要返回三个尺寸的特征图，所以需要这样写：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">forward</span>(<span class=\"params\">self, x</span>):</span></span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">    前向传播</span></span><br><span class=\"line\"><span class=\"string\">    :param x: 输入图</span></span><br><span class=\"line\"><span class=\"string\">    :return: 三个特征图</span></span><br><span class=\"line\"><span class=\"string\">    &quot;&quot;&quot;</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># 传统艺能</span></span><br><span class=\"line\">    x = self.conv1(x)</span><br><span class=\"line\">    x = self.bn1(x)</span><br><span class=\"line\">    x = self.relu1(x)</span><br><span class=\"line\"></span><br><span class=\"line\">    x = self.layer1(x)</span><br><span class=\"line\">    x = self.layer2(x)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># 第一特征图</span></span><br><span class=\"line\">    out3 = self.layer3(x)</span><br><span class=\"line\">    <span class=\"comment\"># 第二特征图</span></span><br><span class=\"line\">    out4 = self.layer4(out3)</span><br><span class=\"line\">    <span class=\"comment\"># 第三特征图</span></span><br><span class=\"line\">    out5 = self.layer5(out4)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">return</span> out3, out4, out5</span><br></pre></td></tr></table></figure>\n<h3 id=\"从特征获取预测结果\"><a href=\"#从特征获取预测结果\" class=\"headerlink\" title=\"从特征获取预测结果\"></a>从特征获取预测结果</h3><p>特征图出来了，但我们需要将特征图转换成最终结果。</p>\n<p>在放代码之前，自定义一套卷积，这是方便后面快速使用：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">conv2d</span>(<span class=\"params\">filter_in, filter_out, kernel_size</span>):</span></span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">    私自定义的卷积</span></span><br><span class=\"line\"><span class=\"string\">    :param filter_in: 输入通道数</span></span><br><span class=\"line\"><span class=\"string\">    :param filter_out: 输出通道数</span></span><br><span class=\"line\"><span class=\"string\">    :param kernel_size: 卷积核尺寸</span></span><br><span class=\"line\"><span class=\"string\">    :return:</span></span><br><span class=\"line\"><span class=\"string\">    &quot;&quot;&quot;</span></span><br><span class=\"line\">    pad = (kernel_size - <span class=\"number\">1</span>) // <span class=\"number\">2</span> <span class=\"keyword\">if</span> kernel_size <span class=\"keyword\">else</span> <span class=\"number\">0</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> nn.Sequential(OrderedDict([</span><br><span class=\"line\">        (<span class=\"string\">&quot;conv&quot;</span>, nn.Conv2d(filter_in, filter_out, kernel_size=kernel_size, stride=<span class=\"number\">1</span>, padding=pad, bias=<span class=\"literal\">False</span>)),</span><br><span class=\"line\">        (<span class=\"string\">&quot;bn&quot;</span>, nn.BatchNorm2d(filter_out)),</span><br><span class=\"line\">        (<span class=\"string\">&quot;relu&quot;</span>, nn.LeakyReLU(<span class=\"number\">0.1</span>)),</span><br><span class=\"line\">    ]))</span><br></pre></td></tr></table></figure>\n<p>这个卷积相当于一套完整的卷积层，但官方这里没有使用残差网络，这有点让我感到疑惑，因为我担心这样会不会影响整个网络前部分的梯度计算。</p>\n<h4 id=\"主体部分的初始化\"><a href=\"#主体部分的初始化\" class=\"headerlink\" title=\"主体部分的初始化\"></a>主体部分的初始化</h4><p>因为是主体部分，所以首先创建yolo主体：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">YoloBody</span>(<span class=\"params\">nn.Module</span>):</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span>(<span class=\"params\">self, config</span>):</span></span><br><span class=\"line\">        <span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">        yolo主体</span></span><br><span class=\"line\"><span class=\"string\">        :param config:</span></span><br><span class=\"line\"><span class=\"string\">        &quot;&quot;&quot;</span></span><br><span class=\"line\">        <span class=\"built_in\">super</span>(YoloBody, self).__init__()</span><br><span class=\"line\">        self.config = config</span><br></pre></td></tr></table></figure>\n<p>然后获取已经创建好的网络（也就是上面的程序）：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 获取darknet53堆叠网络</span></span><br><span class=\"line\">self.backbone = darknet53(<span class=\"literal\">None</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 获取输出通道数</span></span><br><span class=\"line\">out_filters = self.backbone.layers_out_filters</span><br></pre></td></tr></table></figure>\n<p>接下来就是$13*13$特征层的提取：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># last_layer0</span></span><br><span class=\"line\"><span class=\"comment\"># 此值为75</span></span><br><span class=\"line\">final_out_filter0 = <span class=\"built_in\">len</span>(config[<span class=\"string\">&quot;yolo&quot;</span>][<span class=\"string\">&quot;anchors&quot;</span>][<span class=\"number\">0</span>]) * (<span class=\"number\">5</span> + config[<span class=\"string\">&quot;yolo&quot;</span>][<span class=\"string\">&quot;classes&quot;</span>])</span><br><span class=\"line\"><span class=\"comment\"># 5次卷积+2次卷积</span></span><br><span class=\"line\">self.last_layer0 = make_last_layers([<span class=\"number\">512</span>, <span class=\"number\">1024</span>], out_filters[-<span class=\"number\">1</span>], final_out_filter0)</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<p>现在详细说一下<code>final_out_filter0</code>这个变量特点，此变量分两部分，两部分相乘才出结果。<code>len(config[&quot;yolo&quot;][&quot;anchors&quot;][0])</code>是<strong>先验框</strong>个数，我们使用了3个，所以此结果是3；<code>(5 + config[&quot;yolo&quot;][&quot;classes&quot;])</code>这个是$20 + 1 + 4$，也就是25。最终结果就是$(20+1+4)*3=75$。</p>\n<p>这个时候有个<code>make_last_layers</code>函数，这就是5次卷积和最后结果卷积。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">make_last_layers</span>(<span class=\"params\">filters_list, in_filters, out_filter</span>):</span></span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">    5次卷积+2次卷积</span></span><br><span class=\"line\"><span class=\"string\">    :param filters_list:中间过渡通道数</span></span><br><span class=\"line\"><span class=\"string\">    :param in_filters: 输入通道数</span></span><br><span class=\"line\"><span class=\"string\">    :param out_filter: 输出通道数</span></span><br><span class=\"line\"><span class=\"string\">    :return: 5次卷积+2次卷积的模型</span></span><br><span class=\"line\"><span class=\"string\">    &quot;&quot;&quot;</span></span><br><span class=\"line\">    m = nn.ModuleList([</span><br><span class=\"line\">        <span class=\"comment\"># 1*1卷积调整通道（降低通道数）</span></span><br><span class=\"line\">        conv2d(in_filters, filters_list[<span class=\"number\">0</span>], <span class=\"number\">1</span>),</span><br><span class=\"line\">        <span class=\"comment\"># 3*3卷积提取特征</span></span><br><span class=\"line\">        conv2d(filters_list[<span class=\"number\">0</span>], filters_list[<span class=\"number\">1</span>], <span class=\"number\">3</span>),</span><br><span class=\"line\">        <span class=\"comment\"># 1*1卷积调整通道（降低通道数）</span></span><br><span class=\"line\">        conv2d(filters_list[<span class=\"number\">1</span>], filters_list[<span class=\"number\">0</span>], <span class=\"number\">1</span>),</span><br><span class=\"line\">        <span class=\"comment\"># 3*3卷积提取特征</span></span><br><span class=\"line\">        conv2d(filters_list[<span class=\"number\">0</span>], filters_list[<span class=\"number\">1</span>], <span class=\"number\">3</span>),</span><br><span class=\"line\">        <span class=\"comment\"># 1*1卷积调整通道</span></span><br><span class=\"line\">        conv2d(filters_list[<span class=\"number\">1</span>], filters_list[<span class=\"number\">0</span>], <span class=\"number\">1</span>),</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># 接下来两此卷积用来分类预测和回归预测</span></span><br><span class=\"line\">        conv2d(filters_list[<span class=\"number\">0</span>], filters_list[<span class=\"number\">1</span>], <span class=\"number\">3</span>),</span><br><span class=\"line\">        nn.Conv2d(filters_list[<span class=\"number\">1</span>], out_filter, kernel_size=<span class=\"number\">1</span>,</span><br><span class=\"line\">                                        stride=<span class=\"number\">1</span>, padding=<span class=\"number\">0</span>, bias=<span class=\"literal\">True</span>)</span><br><span class=\"line\">    ])</span><br><span class=\"line\">    <span class=\"keyword\">return</span> m</span><br></pre></td></tr></table></figure>\n<p>$1*1$卷积是很有效减少通道数，从而减少参数，这对电脑减轻了不小负担。</p>\n<p>接下来是$13*13$特征层的提取：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># embedding1</span></span><br><span class=\"line\"><span class=\"comment\"># 此值为75</span></span><br><span class=\"line\">final_out_filter1 = <span class=\"built_in\">len</span>(config[<span class=\"string\">&quot;yolo&quot;</span>][<span class=\"string\">&quot;anchors&quot;</span>][<span class=\"number\">1</span>]) * (<span class=\"number\">5</span> + config[<span class=\"string\">&quot;yolo&quot;</span>][<span class=\"string\">&quot;classes&quot;</span>])</span><br><span class=\"line\"><span class=\"comment\"># 1*1卷积调整通道数（从512调节成256通道）</span></span><br><span class=\"line\">self.last_layer1_conv = conv2d(<span class=\"number\">512</span>, <span class=\"number\">256</span>, <span class=\"number\">1</span>)</span><br><span class=\"line\"><span class=\"comment\"># 上采样</span></span><br><span class=\"line\">self.last_layer1_upsample = nn.Upsample(scale_factor=<span class=\"number\">2</span>, mode=<span class=\"string\">&#x27;nearest&#x27;</span>)</span><br><span class=\"line\"><span class=\"comment\"># 5次卷积+2次卷积</span></span><br><span class=\"line\">self.last_layer1 = make_last_layers([<span class=\"number\">256</span>, <span class=\"number\">512</span>], out_filters[-<span class=\"number\">2</span>] + <span class=\"number\">256</span>, final_out_filter1)</span><br></pre></td></tr></table></figure>\n<p>在使用上采样之前需要用$1*1$卷积核来调整通道，这样可以保证接下来上采样时通道一致。</p>\n<p>由于使用上采样，所以我们需要上采样函数，幸运的是pytorch提供了上采样，所以我们免去了这个过程。</p>\n<p>最后就是剩下特征层，也是同样道理：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># embedding2</span></span><br><span class=\"line\"><span class=\"comment\"># 此值为75</span></span><br><span class=\"line\">final_out_filter2 = <span class=\"built_in\">len</span>(config[<span class=\"string\">&quot;yolo&quot;</span>][<span class=\"string\">&quot;anchors&quot;</span>][<span class=\"number\">2</span>]) * (<span class=\"number\">5</span> + config[<span class=\"string\">&quot;yolo&quot;</span>][<span class=\"string\">&quot;classes&quot;</span>])</span><br><span class=\"line\"><span class=\"comment\"># 1*1卷积调整通道数（从256调节成128通道）</span></span><br><span class=\"line\">self.last_layer2_conv = conv2d(<span class=\"number\">256</span>, <span class=\"number\">128</span>, <span class=\"number\">1</span>)</span><br><span class=\"line\"><span class=\"comment\"># 上采样</span></span><br><span class=\"line\">self.last_layer2_upsample = nn.Upsample(scale_factor=<span class=\"number\">2</span>, mode=<span class=\"string\">&#x27;nearest&#x27;</span>)</span><br><span class=\"line\"><span class=\"comment\"># 5次卷积+2次卷积</span></span><br><span class=\"line\">self.last_layer2 = make_last_layers([<span class=\"number\">128</span>, <span class=\"number\">256</span>], out_filters[-<span class=\"number\">3</span>] + <span class=\"number\">128</span>, final_out_filter2)</span><br></pre></td></tr></table></figure>\n<h4 id=\"前向传播-1\"><a href=\"#前向传播-1\" class=\"headerlink\" title=\"前向传播\"></a>前向传播</h4><p>看到这里，所有初始化都完成了，也就是网络结构部分完成了，剩下就是其他的代码，就比如前向传播的整体还没写。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">forward</span>(<span class=\"params\">self, x</span>):</span></span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">    前向传播</span></span><br><span class=\"line\"><span class=\"string\">    :param x: 输入</span></span><br><span class=\"line\"><span class=\"string\">    :return:</span></span><br><span class=\"line\"><span class=\"string\">    &quot;&quot;&quot;</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># 函数中定义函数</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">_branch</span>(<span class=\"params\">last_layer, layer_in</span>):</span></span><br><span class=\"line\">        <span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">        分开卷积过程</span></span><br><span class=\"line\"><span class=\"string\">        :param last_layer: 上一层结果</span></span><br><span class=\"line\"><span class=\"string\">        :param layer_in:会存入卷积过程的结果并会返回出去</span></span><br><span class=\"line\"><span class=\"string\">        :return: out_branch是最后结果</span></span><br><span class=\"line\"><span class=\"string\">        &quot;&quot;&quot;</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> i, e <span class=\"keyword\">in</span> <span class=\"built_in\">enumerate</span>(last_layer):</span><br><span class=\"line\">            layer_in = e(layer_in)</span><br><span class=\"line\">            <span class=\"keyword\">if</span> i == <span class=\"number\">4</span>:</span><br><span class=\"line\">                <span class=\"comment\"># 第五次卷积的时候取出来</span></span><br><span class=\"line\">                out_branch = layer_in</span><br><span class=\"line\">        <span class=\"keyword\">return</span> layer_in, out_branch</span><br></pre></td></tr></table></figure>\n<p>没错，直接套娃就可以，我挺喜欢这样写，因为这样写可以让整个函数在另一个函数周期里，外函数“死亡”时，里面的函数也会“死亡”。之所以定义这个函数是把第五次卷积结果拿出来，因为需要上采样传递给其他层。</p>\n<p>继续写，在DarkNet中我们可以获得到三种特征图。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 获取三种特征图</span></span><br><span class=\"line\">x2, x1, x0 = self.backbone(x)</span><br></pre></td></tr></table></figure>\n<p>现在可以正常正向传播了，首先第一套特征图：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 5次卷积+2次卷积</span></span><br><span class=\"line\"><span class=\"comment\"># 直接可以得到卷积神经网络结果</span></span><br><span class=\"line\">out0, out0_branch = _branch(self.last_layer0, x0)</span><br></pre></td></tr></table></figure>\n<p>第二套特征图使用计算之前，需要调整通道数、上采样、堆叠：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 1*1卷积调整通道数（从512调节成256通道）</span></span><br><span class=\"line\">x1_in = self.last_layer1_conv(out0_branch)</span><br><span class=\"line\"><span class=\"comment\"># 上采样</span></span><br><span class=\"line\">x1_in = self.last_layer1_upsample(x1_in)</span><br><span class=\"line\"><span class=\"comment\"># 堆叠</span></span><br><span class=\"line\">x1_in = torch.cat([x1_in, x1], <span class=\"number\">1</span>)</span><br><span class=\"line\"><span class=\"comment\"># 5次卷积+2次卷积</span></span><br><span class=\"line\"><span class=\"comment\"># 直接可以得到卷积神经网络结果</span></span><br><span class=\"line\">out1, out1_branch = _branch(self.last_layer1, x1_in)</span><br></pre></td></tr></table></figure>\n<p>第三套同样如此：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 1*1卷积调整通道数（从256调节成128通道）</span></span><br><span class=\"line\">x2_in = self.last_layer2_conv(out1_branch)</span><br><span class=\"line\"><span class=\"comment\"># 上采样</span></span><br><span class=\"line\">x2_in = self.last_layer2_upsample(x2_in)</span><br><span class=\"line\"><span class=\"comment\"># 堆叠</span></span><br><span class=\"line\">x2_in = torch.cat([x2_in, x2], <span class=\"number\">1</span>)</span><br><span class=\"line\"><span class=\"comment\"># 5次卷积+2次卷积</span></span><br><span class=\"line\"><span class=\"comment\"># 直接可以得到卷积神经网络结果</span></span><br><span class=\"line\">out2, _ = _branch(self.last_layer2, x2_in)        </span><br></pre></td></tr></table></figure>\n<p>最后直接输出：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">return</span> out0, out1, out2</span><br></pre></td></tr></table></figure>\n<p>以上我们的网络就搭建完毕了，但还缺少解码部分以及损失函数，所以继续写吧。</p>\n<h3 id=\"解码\"><a href=\"#解码\" class=\"headerlink\" title=\"解码\"></a>解码</h3><p>将解码也定义一个层，这样就方便后续的处理：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">DecodeBox</span>(<span class=\"params\">nn.Module</span>):</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span>(<span class=\"params\">self, anchors, num_classes, img_size</span>):</span></span><br><span class=\"line\">        <span class=\"built_in\">super</span>(DecodeBox, self).__init__()</span><br><span class=\"line\">        self.anchors = anchors</span><br><span class=\"line\">        self.num_anchors = <span class=\"built_in\">len</span>(anchors)</span><br><span class=\"line\">        self.num_classes = num_classes</span><br><span class=\"line\">        self.bbox_attrs = <span class=\"number\">5</span> + num_classes</span><br><span class=\"line\">        <span class=\"comment\"># 保存图像大小</span></span><br><span class=\"line\">        self.img_size = img_size</span><br></pre></td></tr></table></figure>\n<p>前向传播部分：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">forward</span>(<span class=\"params\">self, <span class=\"built_in\">input</span></span>):</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># 此时的形状是（N, 3*(20+5), size, size）</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># 获取图片数量（N,）</span></span><br><span class=\"line\">    batch_size = <span class=\"built_in\">input</span>.size(<span class=\"number\">0</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># 获取图片几行网格</span></span><br><span class=\"line\">    input_height = <span class=\"built_in\">input</span>.size(<span class=\"number\">2</span>)</span><br><span class=\"line\">    <span class=\"comment\"># 获取图片几列网格</span></span><br><span class=\"line\">    input_width = <span class=\"built_in\">input</span>.size(<span class=\"number\">3</span>)</span><br></pre></td></tr></table></figure>\n<p>当然还没完，我们还要获得图片步长，也就是图片每个网格内有多少像素点（感受野）：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 计算步长</span></span><br><span class=\"line\"><span class=\"comment\"># 每个网格内有多少像素点</span></span><br><span class=\"line\"><span class=\"comment\"># 图片尺寸 / 每个网格大小</span></span><br><span class=\"line\">stride_h = self.img_size[<span class=\"number\">1</span>] / input_height</span><br><span class=\"line\">stride_w = self.img_size[<span class=\"number\">0</span>] / input_width</span><br></pre></td></tr></table></figure>\n<p>我们之前定义了三种先验框，所以注意最开始的先验框的单位是像素，我们需要将这个改变成根据步长的百分比，也就是归一到特征层上：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 归一到特征层上</span></span><br><span class=\"line\">scaled_anchors = [(anchor_width / stride_w, anchor_height / stride_h) <span class=\"keyword\">for</span> anchor_width, anchor_height <span class=\"keyword\">in</span> self.anchors]</span><br></pre></td></tr></table></figure>\n<p>因为有三套先验框，所以这个循环会执行三次。</p>\n<p>还需要通道转换，<code>.permute(0, 1, 3, 4, 2).contiguous()</code>：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># （N, 3*(20+5), size, size）-&gt;（N, 3 , size, size, 20+5）</span></span><br><span class=\"line\">prediction = <span class=\"built_in\">input</span>.view(batch_size, self.num_anchors,</span><br><span class=\"line\">                        self.bbox_attrs, input_height, input_width).permute(<span class=\"number\">0</span>, <span class=\"number\">1</span>, <span class=\"number\">3</span>, <span class=\"number\">4</span>, <span class=\"number\">2</span>).contiguous()</span><br></pre></td></tr></table></figure>\n<p>处理一下先验框的中心参数，怕是遇到负数，所以使用<code>sigmoid()</code>函数过滤一下，这样百分百是正数且不会大于1：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 先验框的中心位置的调整参数</span></span><br><span class=\"line\">x = torch.sigmoid(prediction[..., <span class=\"number\">0</span>])</span><br><span class=\"line\">y = torch.sigmoid(prediction[..., <span class=\"number\">1</span>])</span><br></pre></td></tr></table></figure>\n<p>还有宽高参数：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 先验框的宽高调整参数</span></span><br><span class=\"line\">w = prediction[..., <span class=\"number\">2</span>]  <span class=\"comment\"># Width</span></span><br><span class=\"line\">h = prediction[..., <span class=\"number\">3</span>]  <span class=\"comment\"># Height</span></span><br></pre></td></tr></table></figure>\n<p>以及物体置信度和种类置信度，听说在之前yolo使用的是<code>softmax()</code>函数，这样就会产生独立事件，有时会出bug，<code>sigmoid()</code>可以解决这个问题：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 获得置信度，是否有物体</span></span><br><span class=\"line\">conf = torch.sigmoid(prediction[..., <span class=\"number\">4</span>])</span><br><span class=\"line\"><span class=\"comment\"># 获得种类置信度</span></span><br><span class=\"line\">pred_cls = torch.sigmoid(prediction[..., <span class=\"number\">5</span>:])</span><br></pre></td></tr></table></figure>\n<p>先验框问题解决了，接下来就生成出来，根据网格左上角生成中心：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 生成网格，先验框中心，网格左上角</span></span><br><span class=\"line\">grid_x = torch.linspace(<span class=\"number\">0</span>, input_width - <span class=\"number\">1</span>, input_width).repeat(input_width, <span class=\"number\">1</span>).repeat(batch_size * self.num_anchors, <span class=\"number\">1</span>, <span class=\"number\">1</span>).view(x.shape).<span class=\"built_in\">type</span>(FloatTensor)</span><br><span class=\"line\">grid_y = torch.linspace(<span class=\"number\">0</span>, input_height - <span class=\"number\">1</span>, input_height).repeat(input_height, <span class=\"number\">1</span>).t().repeat(batch_size * self.num_anchors, <span class=\"number\">1</span>, <span class=\"number\">1</span>).view(y.shape).<span class=\"built_in\">type</span>(FloatTensor)</span><br></pre></td></tr></table></figure>\n<p>也就是从网格的左上角，每隔size单位生成一个，形状是<code>（N, 3, size, size）</code></p>\n<p>再生成先验框的宽和高，根据自己设定的原始先验框：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 生成先验框的宽高</span></span><br><span class=\"line\">anchor_w = FloatTensor(scaled_anchors).index_select(<span class=\"number\">1</span>, LongTensor([<span class=\"number\">0</span>]))</span><br><span class=\"line\">anchor_h = FloatTensor(scaled_anchors).index_select(<span class=\"number\">1</span>, LongTensor([<span class=\"number\">1</span>]))</span><br><span class=\"line\">anchor_w = anchor_w.repeat(batch_size, <span class=\"number\">1</span>).repeat(<span class=\"number\">1</span>, <span class=\"number\">1</span>, input_height * input_width).view(w.shape)</span><br><span class=\"line\">anchor_h = anchor_h.repeat(batch_size, <span class=\"number\">1</span>).repeat(<span class=\"number\">1</span>, <span class=\"number\">1</span>, input_height * input_width).view(h.shape)</span><br></pre></td></tr></table></figure>\n<p>这是最后的调整了：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 计算调整后的先验框中心与宽高</span></span><br><span class=\"line\">pred_boxes = FloatTensor(prediction[..., :<span class=\"number\">4</span>].shape)</span><br><span class=\"line\">pred_boxes[..., <span class=\"number\">0</span>] = x.data + grid_x</span><br><span class=\"line\">pred_boxes[..., <span class=\"number\">1</span>] = y.data + grid_y</span><br><span class=\"line\">pred_boxes[..., <span class=\"number\">2</span>] = torch.exp(w.data) * anchor_w</span><br><span class=\"line\">pred_boxes[..., <span class=\"number\">3</span>] = torch.exp(h.data) * anchor_h</span><br></pre></td></tr></table></figure>\n<p>之前我们获得的先验框中心是一个0到1之间的数字，加上网格的次序就可以直接检查出是哪个网格中的中心。</p>\n<p>结尾：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 用于将输出调整为相对于416x416的大小</span></span><br><span class=\"line\">_scale = torch.Tensor([stride_w, stride_h] * <span class=\"number\">2</span>).<span class=\"built_in\">type</span>(FloatTensor)</span><br><span class=\"line\">output = torch.cat((pred_boxes.view(batch_size, -<span class=\"number\">1</span>, <span class=\"number\">4</span>) * _scale,conf.view(batch_size, -<span class=\"number\">1</span>, <span class=\"number\">1</span>), pred_cls.view(batch_size, -<span class=\"number\">1</span>, self.num_classes)), -<span class=\"number\">1</span>)</span><br><span class=\"line\"><span class=\"keyword\">return</span> output.data</span><br></pre></td></tr></table></figure>\n<blockquote>\n<p>参考：</p>\n<p><a href=\"https://blog.csdn.net/weixin_44791964/article/details/105310627\">https://blog.csdn.net/weixin_44791964/article/details/105310627</a></p>\n<p><a href=\"https://www.bilibili.com/video/BV1Hp4y1y788\">https://www.bilibili.com/video/BV1Hp4y1y788</a></p>\n</blockquote>\n","categories":[{"name":"人工智能","path":"api/categories/人工智能.json"}],"tags":[{"name":"python","path":"api/tags/python.json"},{"name":"深度学习","path":"api/tags/深度学习.json"}]}